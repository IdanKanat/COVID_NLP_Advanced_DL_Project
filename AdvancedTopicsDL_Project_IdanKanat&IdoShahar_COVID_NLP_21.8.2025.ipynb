{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IdanKanat/COVID_NLP_Advanced_DL_Project/blob/main/AdvancedTopicsDL_Project_IdanKanat%26IdoShahar_COVID_NLP_21.8.2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8745014",
      "metadata": {
        "id": "d8745014"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install optuna\n",
        "!pip install wandb\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "1uGV8C_qCHmL"
      },
      "id": "1uGV8C_qCHmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "kxo4w8IFZ0Kk"
      },
      "id": "kxo4w8IFZ0Kk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zubYPI2MZ1BS"
      },
      "id": "zubYPI2MZ1BS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b039d84",
      "metadata": {
        "id": "9b039d84"
      },
      "outputs": [],
      "source": [
        "# Relevant imports:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch import nn, optim\n",
        "import evaluate\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback, get_scheduler\n",
        "from torch.nn.utils import prune\n",
        "\n",
        "import optuna\n",
        "import wandb\n",
        "from datasets import Dataset, DatasetDict, load_from_disk, Value, Sequence, concatenate_datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca02ad3",
      "metadata": {
        "id": "fca02ad3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636c54ac",
      "metadata": {
        "id": "636c54ac"
      },
      "source": [
        "# **Part A - Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Path (Relevant for running the files not from Drive) - **PLEASE FIRST DOWNLOAD THE [Project_COVID_NLP folder](https://drive.google.com/drive/folders/1egGGJ6F878xIk_bKUfjhyZStESiliwRC?usp=sharing) accessible from idankanat@gmail.com's Google drive!!**"
      ],
      "metadata": {
        "id": "7yOqlHzaTgsb"
      },
      "id": "7yOqlHzaTgsb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Drive path we used for this project. Assuming Google Colab exists as well as mounting files to drive, user can change it accordingly as he downloads the Project_COVID_NLP folder as specified in the project_root below and documented above.\n",
        "basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!!\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "data_path = f\"{project_root}/data\""
      ],
      "metadata": {
        "id": "BDFHYsAkTgGo"
      },
      "id": "BDFHYsAkTgGo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5771e368",
      "metadata": {
        "id": "5771e368"
      },
      "outputs": [],
      "source": [
        "# Loading the Corona_NLP_train dataset:\n",
        "df = pd.read_csv(f\"{data_path}/Corona_NLP_train.csv\", encoding='latin1')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "vkf5U_9pN-Jn"
      },
      "id": "vkf5U_9pN-Jn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "587fb095",
      "metadata": {
        "id": "587fb095"
      },
      "source": [
        "### **Sentiments Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d72afabf",
      "metadata": {
        "id": "d72afabf"
      },
      "outputs": [],
      "source": [
        "# Count sentiment frequencies\n",
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "# Define the custom order\n",
        "custom_order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "\n",
        "# Reindex according to desired order\n",
        "sentiment_counts = sentiment_counts.reindex(custom_order)\n",
        "\n",
        "# Format long labels to be multi-line\n",
        "sentiment_counts.index = sentiment_counts.index.str.replace(\"Extremely Positive\", \"Extremely\\nPositive\")\n",
        "sentiment_counts.index = sentiment_counts.index.str.replace(\"Extremely Negative\", \"Extremely\\nNegative\")\n",
        "\n",
        "# Plotting the general sentiment distribution\n",
        "ax = sentiment_counts.plot(kind='bar', color='blue', edgecolor='black')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Sentiment Distribution\", fontweight='bold')\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Add top margin so numbers don't touch the edge\n",
        "plt.ylim(0, sentiment_counts.max() + 1500)\n",
        "\n",
        "# Add bold value labels for each sentiment category, with comma formatting\n",
        "for i, count in enumerate(sentiment_counts):\n",
        "    if pd.notna(count):\n",
        "        plt.text(i, count + 200, f\"{int(count):,}\", ha='center', va='bottom', fontsize=10, fontweight = \"bold\")\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a70dce6",
      "metadata": {
        "id": "2a70dce6"
      },
      "source": [
        "From the sentiment distribution shown above, we can draw a few conclusions:\n",
        "1. **There are more positive tweets than negative tweets.**\n",
        "2. **There are more extremely positive tweets than extremely negative tweets.** This ensures that even when combining the extremes of each sentiment, positive tweets outnumber negatives. The gap between positive and negative tweets enlarges as we add the extremes of each group."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5959189a",
      "metadata": {
        "id": "5959189a"
      },
      "source": [
        "### **Daily Tweet Counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632b0ec6",
      "metadata": {
        "id": "632b0ec6"
      },
      "outputs": [],
      "source": [
        "# Standardize the 'TweetAt' date column:\n",
        "df['TweetAt'] = pd.to_datetime(df['TweetAt'], dayfirst=False, errors='coerce')\n",
        "df = df.dropna(subset=['TweetAt'])\n",
        "\n",
        "# Create a new column 'YearMonth' for grouping by month\n",
        "df['YearDay'] = df['TweetAt'].dt.date\n",
        "\n",
        "# Add a column for tweet length:\n",
        "df['TweetLength'] = df['OriginalTweet'].astype(str).apply(len)\n",
        "\n",
        "# Define sentiment colors:\n",
        "sentiment_order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "colors = {\n",
        "    'Extremely Negative': '#e74c3c',\n",
        "    'Negative': '#e67e22',\n",
        "    'Neutral': '#f1c40f',\n",
        "    'Positive': '#2ecc71',\n",
        "    'Extremely Positive': '#3498db',\n",
        "    'All Tweets': 'gray'\n",
        "}\n",
        "\n",
        "# First, plotting all tweets:\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "day_counts_all = df.groupby('YearDay').size()\n",
        "x_all = day_counts_all.index.astype(str)\n",
        "y_all = day_counts_all.values\n",
        "ax.plot(x_all, y_all, color='gray', marker='o', linewidth=2)\n",
        "ax.set_title(\"Daily Tweet Counts\", fontweight='bold', fontsize=14, pad=20)\n",
        "ax.set_xlabel(\"Day\")\n",
        "ax.set_ylabel(\"Tweet Count\")\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "# for spine in ax.spines.values():\n",
        "    # spine.set_edgecolor('red')\n",
        "    # spine.set_linewidth(3)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Second, plotting stratified sentiment trends in one plot:\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "for sentiment in sentiment_order:\n",
        "    data = df[df['Sentiment'] == sentiment]\n",
        "    day_counts = data.groupby('YearDay').size()\n",
        "    x = day_counts.index.astype(str)\n",
        "    y = day_counts.values\n",
        "    ax.plot(x, y, label=sentiment, color=colors[sentiment], marker='o', linewidth=2)\n",
        "\n",
        "ax.set_title(\"Daily Tweet Counts by Sentiment\", fontweight='bold', fontsize=14, pad=20)\n",
        "ax.set_xlabel(\"Day\")\n",
        "ax.set_ylabel(\"Tweet Count\")\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "ax.legend(title=\"Sentiment\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68c4d84",
      "metadata": {
        "id": "d68c4d84"
      },
      "source": [
        "From the two plots above, we can conclude:\n",
        "- As we could intuitively predict, there was a surge of tweets in March 2020 following the COVID-19 outburst.\n",
        "- This massive surge in tweets wasn't attributed to any specific sentiment but rather all different sentiments indicated much more frequent tweets in March."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2db8fe7",
      "metadata": {
        "id": "f2db8fe7"
      },
      "source": [
        "### **Tweet Length Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb461b0",
      "metadata": {
        "id": "1fb461b0"
      },
      "outputs": [],
      "source": [
        "# Compute the number of characters in each tweet\n",
        "df['TweetLength'] = df['OriginalTweet'].astype(str).str.len()\n",
        "\n",
        "# Plotting a histogram of tweet lengths:\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(df['TweetLength'], bins=50, color='purple', edgecolor='black')\n",
        "\n",
        "plt.title(\"Distribution of Tweet Lengths (in characters)\", fontweight='bold')\n",
        "plt.xlabel(\"Number of Characters\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4074e144",
      "metadata": {
        "id": "4074e144"
      },
      "outputs": [],
      "source": [
        "# Displaying summary statistics using the .describe() command:\n",
        "length_stats = df['TweetLength'].describe().astype(int)\n",
        "length_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c76c0bd",
      "metadata": {
        "id": "7c76c0bd"
      },
      "source": [
        "From the tweet length distributions, several conclusions can be drawn:\n",
        "- **Strong right skew up to the character limit -** There’s a visible increase in tweet counts as length increases, peaking around 240–280 characters.\n",
        "\n",
        "- **A sharp drop after ~280 characters -** Reflects the Twitter character limit (likely 280) — tweets can't go longer, so the distribution is naturally cut off there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2718f9dc",
      "metadata": {
        "id": "2718f9dc"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# Sentiment labels and colors\n",
        "sentiment_order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "colors = {\n",
        "    'Extremely Negative': '#e74c3c',\n",
        "    'Negative': '#e67e22',\n",
        "    'Neutral': '#f1c40f',\n",
        "    'Positive': '#2ecc71',\n",
        "    'Extremely Positive': '#3498db',\n",
        "    'All Tweets': 'gray'\n",
        "}\n",
        "\n",
        "# Manual plot order with 'All Tweets' in the center\n",
        "plot_order = [\n",
        "    'Extremely Negative', 'All Tweets', 'Negative',\n",
        "    'Extremely Positive',            'Neutral',   'Positive'\n",
        "]\n",
        "\n",
        "# Create 2x3 subplots\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each distribution\n",
        "for i, label in enumerate(plot_order):\n",
        "    if label == 'All Tweets':\n",
        "        data = df['TweetLength']\n",
        "    else:\n",
        "        data = df[df['Sentiment'] == label]['TweetLength']\n",
        "\n",
        "    axes[i].hist(data, bins=40, color=colors[label], edgecolor='black', alpha=0.9)\n",
        "    axes[i].set_title(label, fontweight='bold')\n",
        "    axes[i].set_xlabel(\"Tweet Length (characters)\")\n",
        "    axes[i].set_ylabel(\"Count\")\n",
        "    axes[i].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Add bold border to the axes itself (cleaner than external patch)\n",
        "    if label == 'All Tweets':\n",
        "        for spine in axes[i].spines.values():\n",
        "            spine.set_edgecolor('red')\n",
        "            spine.set_linewidth(3)\n",
        "\n",
        "\n",
        "# Title and layout\n",
        "plt.suptitle(\"Tweet Length Distributions by Sentiment\", fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f5803d",
      "metadata": {
        "id": "44f5803d"
      },
      "source": [
        "Comapring the stratified distribution charts above to the general tweet-length distribution, a few insights emerge:\n",
        "1. The distributions of EACH of the non-neutral sentiments (i.e. both positive, negative, and extreme sentiments) seems to ***largely*** align with the general tweet length distribution - right skewed - i.e. a tail to the left. Long tweets are frequent.\n",
        "2. The only distinctfully different stratified histogram is w.r.t to the ***neutral*** sentiment, where shorter tweet lengths are also common, as well as the longer tweets (which are frequent in the other histograms too)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5492a66a",
      "metadata": {
        "id": "5492a66a"
      },
      "outputs": [],
      "source": [
        "# Ensure tweet lengths are computed\n",
        "df['TweetLength'] = df['OriginalTweet'].astype(str).str.len()\n",
        "\n",
        "# Define sentiment order + 'All'\n",
        "all_labels = sentiment_order + ['All Tweets']\n",
        "\n",
        "# Initialize dictionary to collect describe stats\n",
        "summary_dict = {}\n",
        "\n",
        "# Add describe() for each sentiment\n",
        "for sentiment in sentiment_order:\n",
        "    stats = df[df['Sentiment'] == sentiment]['TweetLength'].describe().astype(int)\n",
        "    summary_dict[sentiment] = stats\n",
        "\n",
        "# Add general (all tweets) stats\n",
        "summary_dict['All Tweets'] = df['TweetLength'].describe().astype(int)\n",
        "\n",
        "# Combine into a DataFrame\n",
        "summary_df = pd.DataFrame(summary_dict)\n",
        "\n",
        "# Optional: Reorder rows (metrics)\n",
        "summary_df = summary_df.reindex(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
        "\n",
        "# Highlight function\n",
        "def highlight_extremes(row):\n",
        "    is_max = row == row.max()\n",
        "    is_min = row == row.min()\n",
        "    return ['background-color: lightgreen' if v else\n",
        "            'background-color: salmon' if m else '' for v, m in zip(is_max, is_min)]\n",
        "\n",
        "# Styling the dataframe w.r.t to row's maximum (green) & minimum (red)\n",
        "styled_df = summary_df.style.apply(highlight_extremes, axis=1)\n",
        "\n",
        "styled_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c8608a",
      "metadata": {
        "id": "73c8608a"
      },
      "source": [
        "This table displays the key statistics of each of the stratified distributions (w.r.t to sentiment), as well as the general tweet length distribution. We can observe:\n",
        "\n",
        "- The longest tweet belongs to the extremely negative sentiment group (355 tokens!), the longest extremely positive tweet consisted of 338 tokens, indicating that tweets invoking extreme emotions appear to be longer and they're potentially POSITIVELY associated with length.\n",
        "- The neutral sentiment distribution has the largest S.D, aligning with bigger spread than the other distributions, as described above in the graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6a7015",
      "metadata": {
        "id": "be6a7015"
      },
      "source": [
        "### **Tweets by Region**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c86c87",
      "metadata": {
        "id": "79c86c87"
      },
      "outputs": [],
      "source": [
        "# Load the location data\n",
        "df['Location'] = df['Location'].fillna(\"\").str.lower()  # Standardize by lowercasing all location values\n",
        "\n",
        "# Group key-words by region:\n",
        "region_keywords = {\n",
        "    \"US\": [\n",
        "        \"usa\", \"u.s.a\", \"u.s\", \"america\", \"united states of america\", \"united states\", \"texas\", \"tx\", \"austin\",\n",
        "        \"houston\", \"abilene\", \"new york\", \"new york city\", \"nyc\", \"ny\", \"california\", \"ca\", \"florida\", \"fl\",\n",
        "        \"washington\", \"dc\", \"washington dc\", \"washington d.c.\", \"alaska\", \"chicago\", \"illinois\", \"arizona\", \"az\",\n",
        "        \"atlanta\", \"ga\", \"baltimore\", \"boston\", \"brooklyn\", \"manhattan\", \"queens\", \"bronx\", \"staten island\",\n",
        "        \"il\", \"nc\", \"nj\", \"va\", \"tn\", \"oh\", \"ohio\", \"sc\", \"co\", \"colorado\", \"detroit\", \"mi\", \"hollywood\",\n",
        "        \"los angeles\", \"san fransisco\", \"honolulu\", \"hi\", \"indiana\", \"in\", \"kansas\", \"philadelphia\", \"pa\",\n",
        "        \"phoenix\", \"me\", \"or\", \"portland\", \"oregon\", \"las vegas\", \"nv\", \"maryland\", \"nevada\", \"massachusetts\",\n",
        "        \"miami\", \"michigan\", \"minneapolis\", \"nashville\", \"new orleans\", \"new jersey\", \"salt lake city\", \"ut\",\n",
        "        \"utah\", \"slc\", \"san diego\", \"seattle\", \"silicon valley\"\n",
        "    ],\n",
        "    \"UK & Commonwealth\": [\n",
        "        \"england\", \"uk\", \"u.k\", \"united kingdom\", \"london\", \"essex\", \"leeds\", \"liverpool\", \"manchester\",\n",
        "        \"canada\", \"toronto\", \"ontario\", \"alberta\", \"british columbia\", \"montreal\", \"quebec\", \"ottawa\", \"vancouver\",\n",
        "        \"australia\", \"south australia\", \"canberra\", \"melbourne\", \"sydney\", \"adelaide\", \"victoria\",\n",
        "        \"new zealand\", \"auckland\", \"scotland\", \"aberdeen\", \"edinburgh\", \"glasgow\", \"ireland\", \"dublin\"\n",
        "    ],\n",
        "    \"Europe\": [\n",
        "        \"netherlands\", \"amsterdam\", \"nederland\", \"holland\", \"the netherlands\",\n",
        "        \"germany\", \"berlin\", \"frankfurt\", \"munich\", \"hamburg\", \"dusseldorf\", \"deutschland\",\n",
        "        \"france\", \"paris\", \"belgium\", \"brussels\", \"switzerland\", \"geneva\", \"zurich\",\n",
        "        \"spain\", \"barcelona\", \"madrid\", \"italy\", \"milan\", \"milano\", \"rome\", \"roma\",\n",
        "        \"portugal\", \"lisbon\", \"austria\", \"vienna\", \"russia\", \"moscow\", \"st. petersburg\"\n",
        "    ],\n",
        "    \"Africa\": [\n",
        "        \"south africa\", \"cape town\", \"johannesburg\", \"ghana\", \"accra\", \"nigeria\", \"lagos\",\n",
        "        \"kenya\", \"uganda\", \"kampala\"\n",
        "    ],\n",
        "    \"Asia\": [\n",
        "        \"india\", \"mumbai\", \"new delhi\", \"delhi\", \"bangalore\", \"hong kong\", \"singapore\",\n",
        "        \"japan\", \"tokyo\", \"pakistan\", \"malaysia\", \"china\", \"shanghai\",\n",
        "        \"united arab emirates\", \"united arab emirate\", \"abu dhabi\", \"uae\", \"dubai\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Reverse mapping: from keyword to region\n",
        "keyword_to_region = {\n",
        "    keyword: region for region, keywords in region_keywords.items() for keyword in keywords\n",
        "}\n",
        "\n",
        "# Assigning region per location using this function:\n",
        "def assign_region(location):\n",
        "    for keyword, region in keyword_to_region.items():\n",
        "        if keyword in location:\n",
        "            return region\n",
        "    return None\n",
        "\n",
        "# Map the locations to their corresponding defined regions:\n",
        "df['Region'] = df['Location'].apply(assign_region)\n",
        "\n",
        "# Count tweets per region:\n",
        "region_counts = df['Region'].value_counts().reset_index()\n",
        "region_counts.columns = ['Region', 'TweetCount']\n",
        "\n",
        "# Plotting the pie chart:\n",
        "plt.figure(figsize=(9, 9))\n",
        "wedges, texts, autotexts = plt.pie(\n",
        "    region_counts['TweetCount'],\n",
        "    labels=[f\"{region} ({count})\" for region, count in zip(region_counts['Region'], region_counts['TweetCount'])],\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=140,\n",
        "    pctdistance=0.65,       # Move percentage labels further inward\n",
        "    labeldistance=1.15,     # Move labels further out\n",
        "    textprops={'fontsize': 12}\n",
        ")\n",
        "\n",
        "plt.title(\"Tweet Distribution by Region\", fontsize=14, pad=35)\n",
        "plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ef0248",
      "metadata": {
        "id": "70ef0248"
      },
      "source": [
        "The dataset contains 41,158 tweets, but there are 8,594 tweets without location values (~20%).\n",
        "\n",
        "\n",
        "So, out of the remaining 32,564 tweets with location values, we analyzed the location distribution of **86% of them (28,095 tweets)** as shown in the above pie chart.\n",
        "\n",
        "The other 14% were non-indicative locations (gibberish, small & irrelevant cities without countries mentioned, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c9bd26",
      "metadata": {
        "id": "14c9bd26"
      },
      "source": [
        "### **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce noise in the tweet content, we prepared the Corona_NLP dataset (train & test) for sentiment analysis by standardizing the tweet text. This included:\n",
        "\n",
        "- Expanded English contractions (e.g., don’t → do not) to standardize wording.\n",
        "\n",
        "- Replaced URLs and user mentions with placeholders, while simplifying hashtags.\n",
        "\n",
        "- Removed unnecessary punctuation and normalized whitespace.\n",
        "\n",
        "- Lowercased text to ensure consistency across tokens.\n",
        "\n",
        "The clean versions of the train & test datasets with an added CleanTweet column, were saved as new CSV files for further modeling."
      ],
      "metadata": {
        "id": "Y0urSsS6eYq7"
      },
      "id": "Y0urSsS6eYq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f761e2-819c-42c0-9606-d4d8aab827e6",
      "metadata": {
        "id": "e2f761e2-819c-42c0-9606-d4d8aab827e6"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(f\"{data_path}/Corona_NLP_train.csv\", encoding='latin1')\n",
        "test_df = pd.read_csv(f\"{data_path}/Corona_NLP_test.csv\", encoding='latin1')\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Minimal set to avoid external libs. Non-destructive if not present.\n",
        "# We will apply this function inside the next function (clean_tweet_sentiment_friendly)\n",
        "def basic_contractions_expand(text: str) -> str:\n",
        "    mapping = {\n",
        "        \"can't\": \"can not\", \"won't\": \"will not\", \"don't\": \"do not\", \"doesn't\": \"does not\",\n",
        "        \"didn't\": \"did not\", \"i'm\": \"i am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
        "        \"there's\": \"there is\", \"they're\": \"they are\", \"we're\": \"we are\", \"you're\": \"you are\",\n",
        "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
        "        \"shouldn't\": \"should not\", \"couldn't\": \"could not\", \"wouldn't\": \"would not\",\n",
        "        \"i've\": \"i have\", \"we've\": \"we have\", \"they've\": \"they have\", \"who's\": \"who is\",\n",
        "        \"what's\": \"what is\", \"let's\": \"let us\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
        "        \"he's\": \"he is\", \"she's\": \"she is\"\n",
        "    }\n",
        "    # Replace using regex with word boundaries, case-insensitive\n",
        "    def repl(m):\n",
        "        s = m.group(0)\n",
        "        return mapping.get(s.lower(), s)\n",
        "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, mapping.keys())) + r\")\\b\", flags=re.IGNORECASE)\n",
        "    return pattern.sub(repl, text)\n",
        "\n",
        "#the main cleaning function of the dataset\n",
        "def clean_tweet_sentiment_friendly(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    t = text     # Preserve original sentiment cues as much as possible\n",
        "    t = basic_contractions_expand(t)\n",
        "    t = re.sub(r'http\\S+|www\\.\\S+', ' URL ', t)             # URLs -> token\n",
        "    t = re.sub(r'(?<=\\s)RT\\s+', ' ', t)                     # RT markers (Retweet sign) at word boundary\n",
        "    t = re.sub(r'@\\w+', ' @user ', t)                       # Mentions -> @user\n",
        "    t = re.sub(r'#(\\w+)', r'\\1', t)                         # Remove hashtags but keep hashtag word\n",
        "    t = re.sub(r\"[\\\"$%^&*()\\-_=+\\[\\]{};:|/\\\\<>]\", \" \", t)   # strip most punctuation and special characters, BESIDE ! and ?\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()                      # Normalize whitespaces to only one whitespace\n",
        "    t = t.lower()                                           # Lowercase\n",
        "    return t\n",
        "\n",
        "\n",
        "clean_train = train_df.copy()\n",
        "clean_train[\"CleanTweet\"] = clean_train[\"OriginalTweet\"].apply(clean_tweet_sentiment_friendly)\n",
        "clean_test = test_df.copy()\n",
        "clean_test[\"CleanTweet\"] = clean_test[\"OriginalTweet\"].apply(clean_tweet_sentiment_friendly)\n",
        "clean_train.to_csv(f\"{data_path}/CLEAN_Corona_NLP_train.csv\", index=False, encoding=\"utf-8\")\n",
        "clean_test.to_csv(f\"{data_path}/CLEAN_Corona_NLP_test.csv\",  index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Basic inspections of the cleaned train & test datasets:\n",
        "clean_train.head(200)\n",
        "clean_test.head(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da17ba6",
      "metadata": {
        "id": "7da17ba6"
      },
      "source": [
        "# **Part B - Training Pre-Trained HuggingFace models**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0a74f4-3bbb-4330-b317-d5a941205a34",
      "metadata": {
        "id": "eb0a74f4-3bbb-4330-b317-d5a941205a34"
      },
      "source": [
        "### **Data Splitting - Train and Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the original training dataset into training and validation subsets, ensuring stratification which respects the original label / sentiment distributions, now in the new subsets. Besides, the new subsets contained only relevant info for classification, i.e. the cleaned tweet content and the labels / sentiments themselves."
      ],
      "metadata": {
        "id": "PInTPvp7f-4t"
      },
      "id": "PInTPvp7f-4t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac4dec4-4362-44b6-9bdb-6cbf1f96bd85",
      "metadata": {
        "id": "1ac4dec4-4362-44b6-9bdb-6cbf1f96bd85"
      },
      "outputs": [],
      "source": [
        "# load the CLEAN datasets using ISO-8859-1 encoding due to UTF-8 decoding error\n",
        "# We are loading the only two relevant columns for the classification (Sentiment label & Cleaned Tweet content)\n",
        "train_df = pd.read_csv(f\"{data_path}/CLEAN_Corona_NLP_train.csv\", encoding='latin1',usecols=[\"Sentiment\", \"CleanTweet\"] )\n",
        "test_df = pd.read_csv(f\"{data_path}/CLEAN_Corona_NLP_test.csv\", encoding='latin1', usecols=[\"Sentiment\", \"CleanTweet\"])\n",
        "\n",
        "# Fixed label mapping for all of the data before splitting\n",
        "label_order = ['Extremely Negative','Negative','Neutral','Positive','Extremely Positive']\n",
        "label2id = {l:i for i,l in enumerate(label_order)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "test_size = len(test_df)\n",
        "\n",
        "# Split the training data to create a validation set of the same size as the test set, stratification was included to keep the same label distribution across the training & validation subsets\n",
        "train_df_reduced, val_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=test_size, # Validation set's size equals the test set's size\n",
        "    random_state=42,\n",
        "    stratify=train_df['Sentiment'] # Stratified the subsets w.r.t labels - sentiments\n",
        ")\n",
        "\n",
        "train_df_reduced_size = len(train_df_reduced)\n",
        "val_size = len(val_df)\n",
        "total = train_df_reduced_size + val_size + test_size\n",
        "\n",
        "# Create a summary table\n",
        "summary = {\n",
        "    \"Dataset\": [\"Training after splitting\", \"Validation\", \"Test\"],\n",
        "    \"Records\": [train_df_reduced_size, val_size, test_size],\n",
        "    \"Percentage\": [round(100 * train_df_reduced_size / total, 2),\n",
        "                   round(100 * val_size / total, 2),\n",
        "                   round(100 * test_size / total, 2)]\n",
        "}\n",
        "\n",
        "\n",
        "summary_df = pd.DataFrame(summary)\n",
        "summary_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing 2 Models from HuggingFace (HF) - *RoBERTa-Base-Tweet* and *BERTweet-Base***"
      ],
      "metadata": {
        "id": "9f3JdfNhoZFq"
      },
      "id": "9f3JdfNhoZFq"
    },
    {
      "cell_type": "markdown",
      "id": "c983d5c8",
      "metadata": {
        "id": "c983d5c8"
      },
      "source": [
        "### **A Look at the Model (1)** - ***RoBERTa-Base-Tweet***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc1c6608",
      "metadata": {
        "id": "cc1c6608"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model from Hugging Face\n",
        "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
        "tokenizer_twitter_roberta_base = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the first model from HuggingFace - ROBERTA Transformer Encoder, fine-tuned for sentiment analysis from tweets:\n",
        "roberta_tweets_1_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"cardiffnlp/twitter-roberta-base\", num_labels = 5 # 5 labels for the 5 sentiments\n",
        ").to(device)\n",
        "roberta_tweets_1_model # glancing at the model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b40f296",
      "metadata": {
        "id": "5b40f296"
      },
      "source": [
        "## **Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cd94e6",
      "metadata": {
        "id": "96cd94e6"
      },
      "source": [
        "### **Tweet Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac68d5b",
      "metadata": {
        "id": "eac68d5b"
      },
      "outputs": [],
      "source": [
        "# Defining the TweetDataset class with 3 built in functions (init, len and getitem) for integration with the PyTorch DataLoader object\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame containing the data\n",
        "            tokenizer: HuggingFace tokenizer for text processing\n",
        "            max_length (int): Maximum sequence length\n",
        "        \"\"\"\n",
        "\n",
        "        self.texts = dataframe['CleanTweet'].tolist()\n",
        "        self.labels = dataframe['Sentiment'].map(label2id).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"text\": self.texts[idx], \"label\": self.labels[idx]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c09025",
      "metadata": {
        "id": "01c09025"
      },
      "source": [
        "### **Early Stopping Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3037fec",
      "metadata": {
        "id": "e3037fec"
      },
      "outputs": [],
      "source": [
        "# Check for early stopping, applied for regularization. If the relevant validation metric (accuracy) shows no observable\n",
        "# improvement (w.r.t best observed val metric up until now) over several epochs consecutively, model training stops.\n",
        "# This function outputs the best_val_accuracy, epoch & early stop flag for each epoch it's called\n",
        "def early_stop_check(patience, best_val_accuracy, best_val_accuracy_epoch, current_val_accuracy, current_val_accuracy_epoch):\n",
        "    early_stop_flag = False\n",
        "    if current_val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = current_val_accuracy\n",
        "        best_val_accuracy_epoch = current_val_accuracy_epoch\n",
        "    else:\n",
        "        if current_val_accuracy_epoch - best_val_accuracy_epoch > patience:\n",
        "            early_stop_flag = True\n",
        "    return best_val_accuracy, best_val_accuracy_epoch, early_stop_flag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072faeb0",
      "metadata": {
        "id": "072faeb0"
      },
      "source": [
        "## **Model Training**\n",
        "    The train_model_with_hyperparams function trains the model using the given training and validation loaders,\n",
        "    with early stopping.\n",
        "    Logs training and validation performance to Weights & Biases (accuracy, precision, recall, F1-score, and confusion matrix).\n",
        "    Returns the best model validation loss and saves the best model checkpoint per trial.\n",
        "\n",
        "      Args:\n",
        "        model (.from_pretrained): Transformer encoder model, imported from HuggingFace\n",
        "        train_loader (DataLoader): DataLoader for training data\n",
        "        val_loader (DataLoader): DataLoader for validation data\n",
        "        optimizer (torch.optim.Optimizer): Optimizer\n",
        "        criterion (nn.Module): Loss function\n",
        "        epochs (int): Max number of epochs\n",
        "        patience (int): Early stopping patience\n",
        "        trial (optuna.trial.Trial): Current Optuna trial\n",
        "    Returns:\n",
        "        float: Best validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e18eb0",
      "metadata": {
        "id": "e2e18eb0"
      },
      "outputs": [],
      "source": [
        "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
        "    # speed toggles (safe to call each time)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    best_val_accuracy = 0.0 # Initialize best validation accuracy\n",
        "    best_val_accuracy_epoch = 0 # Track epoch with the best validation accuracy\n",
        "    early_stop_flag = False\n",
        "    best_model_state = None # To save the best model (in each trial / final training)\n",
        "\n",
        "    device_ = next(model.parameters()).device  # robust device grab\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train() # Enable training mode\n",
        "        train_loss = 0.0 # Initializing the cumulative training loss for the current epoch to 0.\n",
        "        total_train = 0 # Initialize total_train here\n",
        "        correct_train = 0 # Initialize correct_train here\n",
        "\n",
        "        train_preds = [] # Store predicted classes for metrics\n",
        "        train_targets = []  # Store true labels for metrics\n",
        "\n",
        "        for batch in train_loader: # Iterates over the train_loader, which is a DataLoader object containing batches of training data. Each iteration yields a batch of inputs (images) and corresponding labels (ground-truth classes).\n",
        "            # Non-blocking H2D copies (works best with pin_memory=True on DataLoader)\n",
        "            input_ids      = batch[\"input_ids\"].to(device_, non_blocking=True)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device_, non_blocking=True)\n",
        "            labels         = batch[\"labels\"].to(device_, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
        "\n",
        "\n",
        "            # AMP forward/backward\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits  = outputs.logits\n",
        "                loss    = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            # (Optional) gradient clipping for extra stability:\n",
        "            # scaler.unscale_(optimizer)\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "\n",
        "            # Compute metrics\n",
        "            bs = labels.size(0)\n",
        "            train_loss += loss.item() * bs\n",
        "            total_train += bs\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "            train_preds.extend(preds.detach().cpu().numpy())\n",
        "            train_targets.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        train_loss /= max(total_train, 1)\n",
        "        train_accuracy = correct_train / max(total_train, 1)\n",
        "        train_f1 = f1_score(train_targets, train_preds, average='macro', zero_division=0)\n",
        "        train_precision = precision_score(train_targets, train_preds, average='macro', zero_division=0)\n",
        "        train_recall = recall_score(train_targets, train_preds, average='macro', zero_division=0)\n",
        "\n",
        "        # Validation check\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss_sum = 0.0\n",
        "            total_val = 0\n",
        "            correct_val = 0\n",
        "            val_preds, val_targets = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids      = batch[\"input_ids\"].to(device_, non_blocking=True)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(device_, non_blocking=True)\n",
        "                    labels         = batch[\"labels\"].to(device_, non_blocking=True)\n",
        "\n",
        "                    # AMP also speeds up eval\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                        logits  = outputs.logits\n",
        "                        loss    = criterion(logits, labels)\n",
        "\n",
        "                    bs = labels.size(0)\n",
        "                    val_loss_sum += loss.item() * bs\n",
        "                    total_val += bs\n",
        "\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                    correct_val += (preds == labels).sum().item()\n",
        "                    val_preds.extend(preds.detach().cpu().numpy())\n",
        "                    val_targets.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "            val_loss = val_loss_sum / max(total_val, 1)\n",
        "            val_accuracy = correct_val / max(total_val, 1)\n",
        "            val_precision = precision_score(val_targets, val_preds, average='macro', zero_division=0)\n",
        "            val_recall = recall_score(val_targets, val_preds, average='macro', zero_division=0)\n",
        "            val_f1 = f1_score(val_targets, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "            # Check for Early stopping (& updates best_val_accuracy & epoch)\n",
        "            if patience is not None:\n",
        "                best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
        "                    patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch\n",
        "                )\n",
        "\n",
        "            # Save best-so-far weights (>= to handle ties)\n",
        "            if val_accuracy >= best_val_accuracy and total_val > 0:\n",
        "                best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "            # W & B logging (if active)\n",
        "            if wandb.run is not None:\n",
        "                wandb.log({\n",
        "                    \"Epoch\": epoch,\n",
        "                    \"Train Loss\": train_loss,\n",
        "                    \"Train Accuracy\": train_accuracy,\n",
        "                    \"Train F1 Score\": train_f1,\n",
        "                    \"Train Precision\": train_precision,\n",
        "                    \"Train Recall\": train_recall,\n",
        "                    \"Validation Loss\": val_loss,\n",
        "                    \"Validation Accuracy\": val_accuracy,\n",
        "                    \"Validation Precision\": val_precision,\n",
        "                    \"Validation Recall\": val_recall,\n",
        "                    \"Validation F1\": val_f1,\n",
        "                })\n",
        "\n",
        "            if early_stop_flag:\n",
        "                break\n",
        "\n",
        "    # Save best model weights (if we ever improved)\n",
        "    # if best_model_state is not None:\n",
        "        # torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\")\n",
        "\n",
        "    # Restore best weights into the model before returning best_val_accuracy\n",
        "    if best_model_state is not None:\n",
        "      model.load_state_dict(best_model_state)\n",
        "\n",
        "    return best_val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e7ceeb",
      "metadata": {
        "id": "e4e7ceeb"
      },
      "source": [
        "## **HP Tuning using the Objective function (without HuggingFace's Trainer)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292f0897-77a6-4117-ab44-c420f0142103",
      "metadata": {
        "id": "292f0897-77a6-4117-ab44-c420f0142103"
      },
      "source": [
        "Optuna objective function for tuning the given Transformer encoder model on twitter data.\n",
        "\n",
        "Each trial runs training with a different set of hyperparameters and logs key training & validation metrics to Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cacbbd9-0f22-45bc-8219-b4ff8b4280cf",
      "metadata": {
        "id": "7cacbbd9-0f22-45bc-8219-b4ff8b4280cf"
      },
      "outputs": [],
      "source": [
        "# Objective Function for Optuna:\n",
        "def objective(trial, architecture):\n",
        "\n",
        "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
        "    if architecture == \"twitter-roberta-base\":\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        base_model = model.roberta\n",
        "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
        "    else:\n",
        "        model_name = \"vinai/bertweet-base\"\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        base_model = model.roberta\n",
        "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
        "\n",
        "\n",
        "    # Hyperparameter suggestions:\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
        "    patience = trial.suggest_int(\"patience\", 7, 10)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 3)\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # build loaders from the pretokenized HF dataset\n",
        "    train_loader = DataLoader(\n",
        "        ds[\"train_reduced\"], batch_size=batch_size, shuffle=True,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        ds[\"validation\"], batch_size=min(2*batch_size, 128), shuffle=False,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    #Freezing and Unfreezing layers\n",
        "    for p in base_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    if num_layers_finetune > 0:  # safety guard: avoid the \"-0\" edge case\n",
        "        for p in base_model.encoder.layer[-num_layers_finetune:].parameters():\n",
        "            p.requires_grad = True\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
        "    wandb.init(project=f\"{architecture}_CORONA_NLP_Twitter_Sentiment_Analysis_13.8.2025_FULL_HP_TUNING\",\n",
        "               entity = \"idoshahar96-tel-aviv-university\",\n",
        "               config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"patience\": patience,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"num_layers_finetune\": num_layers_finetune,\n",
        "        \"architecture\": architecture,\n",
        "        \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"},\n",
        "        name=f\"trial_{trial.number}\") # The name that will be saved in the W&B platform\n",
        "\n",
        "    # Train the model and get the best validation accuracy\n",
        "    best_val_accuracy = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=patience, trial=trial)\n",
        "\n",
        "    wandb.finish() # Finish the Weights & Biases run\n",
        "\n",
        "    return best_val_accuracy # Return best validation accuracy as the objective to maximize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d7e457c-416b-4b5c-9a57-2d0bf269113f",
      "metadata": {
        "id": "2d7e457c-416b-4b5c-9a57-2d0bf269113f"
      },
      "source": [
        "## **Pre-tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc9f9df-f196-4a9a-9096-c298dc9679ed",
      "metadata": {
        "id": "afc9f9df-f196-4a9a-9096-c298dc9679ed"
      },
      "source": [
        "Tokenization is CPU-heavy. If we do it all over again in each of the Optuna trials, then re-tokenizing wastes time.\n",
        "Pre-tokenization makes that cost zero for subsequent runs.\n",
        "\n",
        "It includes:\n",
        "running the tokenizer once over the whole dataset and applying truncation with a fixed ceiling MAX_LEN.\n",
        "For each sample i, we store a variable-length vector called len_i, which will be the min(original_len_i, MAX_LEN).\n",
        "Each saved sample can have a different length.\n",
        "You save the result to disk (Arrow format) with the columns of input_ids, attention_mask, and labels\n",
        "So, after this step, no trial needs to call the tokenizer and every trial just loads these IDs.\n",
        "\n",
        "In this step we are doing padding at all:\n",
        "The DataLoader pulls a batch from the disk. Then, the collator looks at the lengths in that batch, finds the longest sequence in each batch, and pads only up to this length.\n",
        "This is dynamic padding: it happens per batch, at runtime, and never re-tokenizes—it only adds pad tokens so tensors in the batch share the same shape\n",
        "The Dynamic padding keeps tensors tight to the batch’s real lengths → fewer pad tokens → fewer FLOPs in the model’s forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136c80dd-9014-471d-87cf-d55954a5cbe8",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "136c80dd-9014-471d-87cf-d55954a5cbe8"
      },
      "outputs": [],
      "source": [
        "# Converting the sentiment labels into integers via label2id, and drops the original Sentiment column\n",
        "train_df_reduced_ = train_df_reduced.assign(label=train_df_reduced[\"Sentiment\"].map(label2id)).drop(columns=[\"Sentiment\"])\n",
        "val_df_ = val_df.assign(label=val_df[\"Sentiment\"].map(label2id)).drop(columns=[\"Sentiment\"])\n",
        "test_df_ = test_df.assign(label=test_df[\"Sentiment\"].map(label2id)).drop(columns=[\"Sentiment\"])\n",
        "\n",
        "# SANITY CHECK - to make sure our training works, we added this code to make sure the training works on little training & validation data (as well as few trials & epochs per trial).\n",
        "# train_df_reduced_ = train_df_reduced_.sample(n=300, random_state=42)  # pick only 300 training rows\n",
        "# val_df_ = val_df_.sample(n=100, random_state=42)                      # pick only 100 validation rows\n",
        "# test_df_ = test_df_.sample(n=100, random_state=42)                    # optional: smaller test set too\n",
        "\n",
        "# Converting the Pandas DataFrames to HuggingFace Datasets and wraping them in a DatasetDict\n",
        "raw_ds = DatasetDict({\n",
        "    \"train_reduced\": Dataset.from_pandas(train_df_reduced_, preserve_index=False),\n",
        "    \"validation\": Dataset.from_pandas(val_df_, preserve_index=False),\n",
        "    \"test\": Dataset.from_pandas(test_df_, preserve_index=False),\n",
        "})\n",
        "\n",
        "\n",
        "def pretokenize_one(model_name: str, save_dir: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    # compute a single cap once (same idea you used inside objective)\n",
        "    enc_tmp  = tok(train_df_reduced[\"CleanTweet\"].tolist(), truncation=False)\n",
        "    lengths  = [len(x) for x in enc_tmp[\"input_ids\"]]\n",
        "    MAX_LEN  = max(64, min(int(np.percentile(lengths, 95)), 128))\n",
        "    print(f\"[{model_name}] MAX_LEN={MAX_LEN}\")\n",
        "\n",
        "    # tokenize (NO padding) and save\n",
        "    tokenized = raw_ds.map(\n",
        "        lambda b: tok(b[\"CleanTweet\"], truncation=True, max_length=MAX_LEN, padding=False),\n",
        "        batched=True, remove_columns=[\"CleanTweet\"]\n",
        "    )\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.save_to_disk(save_dir)\n",
        "    print(f\"Saved to: {save_dir}\")\n",
        "\n",
        "# Run once per architecture you plan to use:\n",
        "pretokenize_one(\"cardiffnlp/twitter-roberta-base\", \"data/tokenized_twitter_roberta_base\")\n",
        "pretokenize_one(\"vinai/bertweet-base\",          \"data/tokenized_bertweet_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09dbdca-6989-42bf-bbe6-9fc88f742475",
      "metadata": {
        "id": "c09dbdca-6989-42bf-bbe6-9fc88f742475"
      },
      "source": [
        "### **Running the Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5bd629-4756-4db5-9658-a6d53f33afb8",
      "metadata": {
        "id": "7a5bd629-4756-4db5-9658-a6d53f33afb8"
      },
      "source": [
        "#### ***Model (1) - RoBERTa-Base-Tweet***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37af89ab",
      "metadata": {
        "scrolled": true,
        "id": "37af89ab"
      },
      "outputs": [],
      "source": [
        "# Creating an Optuna Study - RoBERTa-Base-Tweet (rec4):\n",
        "study = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
        "study.optimize(lambda trial: objective(trial, \"twitter-roberta-base\"), n_trials=10) # Specified 10 trials"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Documenting best hyperparameter combination - first model - RoBERTa-Base-Tweet - Rec4 code:\n",
        "study_roberta_base_tweet_rec4 = study\n",
        "print(\"Best objective value (validation accuracy):\", study.best_value)\n",
        "print(\"The chosen HP combination:\", study.best_params)\n",
        "print(\"Trial number of the best objective (validation accuracy) value:\", study.best_trial.number)\n",
        "\n",
        "print(\"Best objective value (validation accuracy):\", study_roberta_base_tweet_rec4.best_value)\n",
        "print(\"The chosen HP combination:\", study_roberta_base_tweet_rec4.best_params)\n",
        "print(\"Trial number of the best objective (validation accuracy) value:\", study_roberta_base_tweet_rec4.best_trial.number)"
      ],
      "metadata": {
        "id": "gy5B8ZvlYO07"
      },
      "id": "gy5B8ZvlYO07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the file in Google Drive with REC4 naming\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "hp_root = f\"{project_root}/Model_HPs\"\n",
        "drive_path = f\"{hp_root}/best_roberta_base_tweet_rec4_hyperparams.json\"\n",
        "\n",
        "with open(drive_path, \"w\") as f:\n",
        "    # json.dump(study_bertweet_base_rec4.best_params, f)\n",
        "    json.dump({'learning_rate': 0.0003834791389042033, 'weight_decay': 2.88286253103848e-06, 'patience': 7, 'batch_size': 128, 'num_layers_finetune': 3}, f) # Manually typed the best_params for future use\n",
        "\n",
        "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
      ],
      "metadata": {
        "id": "ELU2fW_c2WQR"
      },
      "id": "ELU2fW_c2WQR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bc7e814d-6f75-4764-8a27-0b41ab022eb8",
      "metadata": {
        "id": "bc7e814d-6f75-4764-8a27-0b41ab022eb8"
      },
      "source": [
        "#### ***Model (2) - BERTweet-Base***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model from Hugging Face\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "tokenizer_bertweet_base = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the second model from HuggingFace - BERT-TWEET Transformer Encoder, fine-tuned for sentiment analysis from tweets:\n",
        "bertweet_base_2_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"vinai/bertweet-base\", num_labels = 5 # 5 labels for the 5 sentiments\n",
        ").to(device)\n",
        "bertweet_base_2_model # glancing at the model architecture"
      ],
      "metadata": {
        "id": "gQr3Fgv1hzyo"
      },
      "id": "gQr3Fgv1hzyo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08876e11-915b-4afe-9968-6602815b0d7a",
      "metadata": {
        "scrolled": true,
        "id": "08876e11-915b-4afe-9968-6602815b0d7a"
      },
      "outputs": [],
      "source": [
        "# Creating an Optuna Study - BERTweet-Base (rec4):\n",
        "study_bertweet_base_rec4 = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
        "study_bertweet_base_rec4.optimize(lambda trial: objective(trial, \"bertweet-base\"), n_trials=10) # Specified 10 trials"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Documenting best hyperparameter combination - Second Model - BERTweet-Base - Rec4 code:\n",
        "\n",
        "# print(\"Best objective value (validation accuracy):\", study_bertweet_base_rec4.best_value)\n",
        "# print(\"The chosen HP combination:\", study_bertweet_base_rec4.best_params)\n",
        "# print(\"Trial number of the best objective (validation accuracy) value:\", study_bertweet_base_rec4.best_trial.number)\n",
        "\n",
        "# Define the path to save the file in Google Drive with REC4 naming\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "hp_root = f\"{project_root}/Model_HPs\"\n",
        "drive_path = f\"{hp_root}/best_bertweet_base_rec4_hyperparams.json\"\n",
        "\n",
        "with open(drive_path, \"w\") as f:\n",
        "    # json.dump(study_bertweet_base_rec4.best_params, f)\n",
        "    json.dump({'learning_rate': 0.0001184412471705182, 'weight_decay': 1.2699696348040995e-05, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3}, f) # Manually typed the best_params for future use\n",
        "\n",
        "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
      ],
      "metadata": {
        "id": "YCcVM8TQiwri"
      },
      "id": "YCcVM8TQiwri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Training WITHOUT using HuggingFace functions (Trainer)**"
      ],
      "metadata": {
        "id": "eRjvwb0Me7Xu"
      },
      "id": "eRjvwb0Me7Xu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finding the best trial (hyperparameter combination) using the objective function, the `FINAL_train_model_with_hyperparams` is called for final model training using the obtained hyperparameter combination. It appears similar to the way we trained each model under each trial specification in the Optuna based objective function. This additional function supports model saving too, and generalized for each model architecture. It's worth noting that in practice, the validation dataset in this function would be the actual test set."
      ],
      "metadata": {
        "id": "GY3WwP6ufV2m"
      },
      "id": "GY3WwP6ufV2m"
    },
    {
      "cell_type": "code",
      "source": [
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "\n",
        "# Define model_root inside the project, for all trained weights\n",
        "model_root = f\"{project_root}/Model_Weights\""
      ],
      "metadata": {
        "id": "nL8J3iRArTi1"
      },
      "id": "nL8J3iRArTi1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FINAL_train_model_with_hyperparams(architecture, best_params, save_path):\n",
        "\n",
        "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
        "    if architecture == \"twitter-roberta-base\":\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        base_model = model.roberta\n",
        "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
        "    else:\n",
        "        model_name = \"vinai/bertweet-base\"\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        base_model = model.roberta\n",
        "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # Merge train + validation for final training\n",
        "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
        "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
        "\n",
        "    # build loaders from the pretokenized HF dataset\n",
        "    train_loader = DataLoader(\n",
        "        full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        ds[\"test\"], batch_size=min(2*best_params[\"batch_size\"], 128), shuffle=False,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    #Freezing and Unfreezing layers\n",
        "    for p in base_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    if best_params[\"num_layers_finetune\"] > 0:  # safety guard: avoid the \"-0\" edge case\n",
        "        for p in base_model.encoder.layer[-best_params[\"num_layers_finetune\"]:].parameters():\n",
        "            p.requires_grad = True\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"], weight_decay=best_params[\"weight_decay\"])\n",
        "\n",
        "    if wandb.run is not None:\n",
        "      wandb.finish() # Check if W&B doesn't run anything in parallel. If so, stop the pre-existing run.\n",
        "\n",
        "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
        "    wandb.init(project=f\"{architecture}_CORONA_NLP_Twitter_Sentiment_Analysis_19.8.2025_FULL_TRAINING\",\n",
        "               entity = \"idoshahar96-tel-aviv-university\",\n",
        "               config={\n",
        "        \"learning_rate\": best_params[\"learning_rate\"],\n",
        "        \"weight_decay\": best_params[\"weight_decay\"],\n",
        "        \"patience\": best_params[\"patience\"],\n",
        "        \"batch_size\": best_params[\"batch_size\"],\n",
        "        \"num_layers_finetune\": best_params[\"num_layers_finetune\"],\n",
        "        \"architecture\": architecture,\n",
        "        \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"},\n",
        "        name=\"FINAL_TRAINING\", # The name that will be saved in the W&B platform\n",
        "        reinit=True)\n",
        "\n",
        "    # Train the model and get the best validation accuracy\n",
        "    best_val_accuracy = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=25, patience=best_params[\"patience\"], trial=None)\n",
        "\n",
        "    wandb.finish() # Finish the Weights & Biases run\n",
        "\n",
        "   # Save model\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "esqD8VamlW_B"
      },
      "id": "esqD8VamlW_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_params = study_roberta_base_tweet_rec4.best_params  # get best HPs from the model's Optuna study (are under # but the hashtag sign # can be removed if needed, for the sake of manual inscription of best param, check the row below)\n",
        "best_params = {'learning_rate': 0.0003834791389042033, 'weight_decay': 2.88286253103848e-06, 'patience': 7, 'batch_size': 128, 'num_layers_finetune': 3} # Manually typed the best_params for future use\n",
        "name_path = \"/best_model_roberta_base_tweet_rec4\"\n",
        "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
        "\n",
        "# Training the Model (1), using Optuna-study's best trial HPs - RoBERTa-Base-Tweet:\n",
        "FINAL_train_model_with_hyperparams(architecture=\"twitter-roberta-base\", best_params=best_params,save_path=save_path)\n",
        "\n",
        "# Zip the whole model folder\n",
        "shutil.make_archive(save_path, \"zip\", save_path)\n",
        "\n",
        "# Download the zip to your computer\n",
        "files.download(f\"{save_path}.zip\")"
      ],
      "metadata": {
        "id": "31g2wIDZkkWR"
      },
      "id": "31g2wIDZkkWR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_params = study_bertweet_base_rec4.best_params  # get best HPs from the model's Optuna study (are under # but the hashtag sign # can be removed if needed, for the sake of manual inscription of best param, check the row below)\n",
        "best_params = {'learning_rate': 0.0001184412471705182, 'weight_decay': 1.2699696348040995e-05, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3} # Manually typed the best_params for future use\n",
        "name_path = \"/best_model_bertweet_base_rec4\"\n",
        "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
        "\n",
        "# Training the Model (2), using Optuna-study's best trial HPs - BERTweet-Base:\n",
        "FINAL_train_model_with_hyperparams(architecture=\"bertweet-base\", best_params=best_params,save_path=save_path)\n",
        "\n",
        "# Zip the whole model folder\n",
        "shutil.make_archive(save_path, \"zip\", save_path)\n",
        "\n",
        "# Download the zip to your computer\n",
        "files.download(f\"{save_path}.zip\")"
      ],
      "metadata": {
        "id": "aac7xoMXkViY"
      },
      "id": "aac7xoMXkViY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **HP Tuning using HuggingFace functions (Trainer)**"
      ],
      "metadata": {
        "id": "dzBtXCucn2JO"
      },
      "id": "dzBtXCucn2JO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load evaluation metrics, using the evaluate library\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "# Compute metrics function for the Trainer\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"precision\": precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
        "        \"recall\": recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
        "        \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "N75YY4k7JYao"
      },
      "id": "N75YY4k7JYao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective function for Optuna hyperparameter tuning\n",
        "def objective_HF(trial, architecture):\n",
        "\n",
        "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
        "    if architecture == \"twitter-roberta-base\":\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
        "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
        "    else:\n",
        "        model_name = \"vinai/bertweet-base\"\n",
        "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5) # 5 labels for the 5 sentiments\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    base_model = model.roberta # Base model for both models (RoBERTa-Base-Tweet & BERTweet-Base) - RoBERTa\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
        "    patience = trial.suggest_int(\"patience\", 7, 10)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 3)\n",
        "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"polynomial\"])\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # Freezing and Unfreezing layers\n",
        "    for p in base_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    if num_layers_finetune > 0:  # safety guard: avoid the \"-0\" edge case\n",
        "        for p in base_model.encoder.layer[-num_layers_finetune:].parameters():\n",
        "            p.requires_grad = True\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    if wandb.run is not None:\n",
        "      wandb.finish() # Check if W&B doesn't run anything in parallel. If so, stop the pre-existing run.\n",
        "\n",
        "   # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
        "    wandb.init(\n",
        "        project=f\"{architecture}_HF_CORONA_NLP_Twitter_Sentiment_Analysis_14.8.2025_FULL_HP_TUNING\",\n",
        "        entity=\"idoshahar96-tel-aviv-university\",\n",
        "        config={\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"patience\": patience,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"num_layers_finetune\": num_layers_finetune,\n",
        "            \"lr_scheduler_type\": lr_scheduler_type,\n",
        "            \"architecture\": architecture,\n",
        "            \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"\n",
        "        },\n",
        "        name=f\"trial_{trial.number}\",\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # TrainingArguments for the Hugging Face Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"HF-results/trial_{trial.number}\",  # where checkpoints will be saved\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        num_train_epochs=20,           # Setting the number of epochs for training - 20\n",
        "        eval_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
        "        save_strategy=\"epoch\",        # save a checkpoint at the end of each epoch\n",
        "        logging_strategy=\"epoch\",     # log metrics at the end of each epoch\n",
        "        load_best_model_at_end=True,  # reload the best checkpoint (based on metric_for_best_model)\n",
        "        metric_for_best_model=\"accuracy\", # optimize w.r.t accuracy\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,           # keep only the best checkpoint\n",
        "        report_to=\"wandb\",            # log to Weights & Biases\n",
        "        lr_scheduler_type=lr_scheduler_type\n",
        "    )\n",
        "\n",
        "    # Create Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=ds[\"train_reduced\"],\n",
        "        eval_dataset=ds[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save best trial results\n",
        "    trainer.save_model(f\"HF-results/trial_{trial.number}\")  # ensures config.json + weights are there\n",
        "    tokenizer.save_pretrained(f\"HF-results/trial_{trial.number}\")\n",
        "\n",
        "    # Evaluate the best model on the validation set\n",
        "    eval_metrics = trainer.evaluate()\n",
        "    wandb.finish()\n",
        "\n",
        "    # Optuna uses the validation accuracy as the optimization target\n",
        "    acc = eval_metrics.get(\"eval_accuracy\", 0.0)\n",
        "    if np.isnan(acc):\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "gOXEEYcgnwLc"
      },
      "id": "gOXEEYcgnwLc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RoBERTa-Base-Tweet:**"
      ],
      "metadata": {
        "id": "uZ_rBd0wwiCO"
      },
      "id": "uZ_rBd0wwiCO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an Optuna Study - RoBERTa -Base-Tweet (rec5):\n",
        "study_roberta_base_tweet_rec5 = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
        "study_roberta_base_tweet_rec5.optimize(lambda trial: objective_HF(trial, \"twitter-roberta-base\"), n_trials=12) # Specified 12 trials\n",
        "\n",
        "print(\"Best objective value (validation accuracy):\", study_roberta_base_tweet_rec5.best_value)\n",
        "print(\"The chosen HP combination:\", study_roberta_base_tweet_rec5.best_params)\n",
        "print(\"Trial number of the best objective (validation accuracy) value:\", study_roberta_base_tweet_rec5.best_trial.number)\n",
        "\n",
        "# Define the path to save the file in Google Drive with REC5 naming\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "hp_root = f\"{project_root}/Model_HPs\"\n",
        "drive_path = f\"{hp_root}/best_model_roberta_base_tweet_rec5_hyperparams.json\"\n",
        "\n",
        "with open(drive_path, \"w\") as f:\n",
        "    json.dump(study_roberta_base_tweet_rec5.best_params, f)\n",
        "\n",
        "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
      ],
      "metadata": {
        "id": "qMuNLFT1Npp7"
      },
      "id": "qMuNLFT1Npp7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **BerTweet-Base:**"
      ],
      "metadata": {
        "id": "zZCQF523wtvu"
      },
      "id": "zZCQF523wtvu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an Optuna Study - BerTweet-Base (rec5):\n",
        "study_bertweet_base_rec5 = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
        "study_bertweet_base_rec5.optimize(lambda trial: objective_HF(trial, \"bertweet-base\"), n_trials=12) # Specified 12 trials\n",
        "\n",
        "print(\"Best objective value (validation accuracy):\", study_bertweet_base_rec5.best_value)\n",
        "print(\"The chosen HP combination:\", study_bertweet_base_rec5.best_params)\n",
        "print(\"Trial number of the best objective (validation accuracy) value:\", study_bertweet_base_rec5.best_trial.number)\n",
        "\n",
        "# Define the path to save the file in Google Drive with REC5 naming\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "hp_root = f\"{project_root}/Model_HPs\"\n",
        "drive_path = f\"{hp_root}/best_model_bertweet_base_rec5_hyperparams.json\"\n",
        "\n",
        "with open(drive_path, \"w\") as f:\n",
        "    json.dump(study_bertweet_base_rec5.best_params, f)\n",
        "\n",
        "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
      ],
      "metadata": {
        "id": "fePovRGhNfpc"
      },
      "id": "fePovRGhNfpc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Training using HuggingFace (HF) functions**"
      ],
      "metadata": {
        "id": "LAD3Ed_AelJs"
      },
      "id": "LAD3Ed_AelJs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finding the best trial (hyperparameter combination) using the objective-HF function, the `train_model_with_hyperparams_HF` is called for final model training using the obtained hyperparameter combination. It appears similar to the way we trained each model under each trial specification in the Optuna based objective-HF function. This additional function supports model saving too, and generalized for each model architecture. It's worth noting that in practice, the validation dataset in this function would be the actual test set."
      ],
      "metadata": {
        "id": "FKPi7ArMghQx"
      },
      "id": "FKPi7ArMghQx"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_hyperparams_HF(architecture, best_params, save_path):\n",
        "\n",
        "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
        "    if architecture == \"twitter-roberta-base\":\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
        "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
        "    else:\n",
        "        model_name = \"vinai/bertweet-base\"\n",
        "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5) # 5 labels for the 5 sentiments\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    base_model = model.roberta # Base model for both models (RoBERTa-Base-Tweet & BERTweet-Base) - RoBERTa\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # Merge train + validation for final training\n",
        "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
        "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # Freezing and Unfreezing layers\n",
        "    for p in base_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    if best_params[\"num_layers_finetune\"] > 0:  # safety guard: avoid the \"-0\" edge case\n",
        "        for p in base_model.encoder.layer[-best_params[\"num_layers_finetune\"]:].parameters():\n",
        "            p.requires_grad = True\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    if wandb.run is not None:\n",
        "      wandb.finish() # Check if W&B doesn't run anything in parallel. If so, stop the pre-existing run.\n",
        "\n",
        "   # Initialize Weights & Biases - the values in the config are the properties of the best trial found in the Optuna-HP-Tuning step.\n",
        "    wandb.init(\n",
        "        project=f\"{architecture}_HF_CORONA_NLP_Twitter_Sentiment_Analysis_19.8.2025_FULL_TRAINING\",\n",
        "        entity=\"idoshahar96-tel-aviv-university\",\n",
        "        config={\n",
        "            \"learning_rate\": best_params[\"learning_rate\"],\n",
        "            \"weight_decay\": best_params[\"weight_decay\"],\n",
        "            \"patience\": best_params[\"patience\"],\n",
        "            \"batch_size\": best_params[\"batch_size\"],\n",
        "            \"num_layers_finetune\": best_params[\"num_layers_finetune\"],\n",
        "            \"lr_scheduler_type\": best_params[\"lr_scheduler_type\"],\n",
        "            \"architecture\": architecture,\n",
        "            \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"\n",
        "        },\n",
        "        name=\"FINAL_TRAINING\",\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # TrainingArguments for the Hugging Face Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=save_path,  # where checkpoints will be saved\n",
        "        per_device_train_batch_size=best_params[\"batch_size\"],\n",
        "        per_device_eval_batch_size=best_params[\"batch_size\"],\n",
        "        learning_rate=best_params[\"learning_rate\"],\n",
        "        weight_decay=best_params[\"weight_decay\"],\n",
        "        num_train_epochs=25,           # Setting the number of epochs for training - 25\n",
        "        eval_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
        "        save_strategy=\"epoch\",        # save a checkpoint at the end of each epoch\n",
        "        logging_strategy=\"epoch\",     # log metrics at the end of each epoch\n",
        "        load_best_model_at_end=True,  # reload the best checkpoint (based on metric_for_best_model)\n",
        "        metric_for_best_model=\"accuracy\", # optimize w.r.t accuracy\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,           # keep only the best checkpoint\n",
        "        report_to=\"wandb\",            # log to Weights & Biases\n",
        "        lr_scheduler_type=best_params[\"lr_scheduler_type\"]\n",
        "    )\n",
        "\n",
        "    # Create Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=full_train_dataset,\n",
        "        eval_dataset=ds[\"test\"], # Evaluating the model using the test dataset\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=best_params[\"patience\"])]\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    trainer.save_model(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "vBgPa9QLTUUx"
      },
      "id": "vBgPa9QLTUUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RoBERTa-Base-Tweet:**"
      ],
      "metadata": {
        "id": "Yqao3LoXiH_O"
      },
      "id": "Yqao3LoXiH_O"
    },
    {
      "cell_type": "code",
      "source": [
        "# best_params = study_roberta_base_tweet_rec5.best_params  # get best HPs from the model's Optuna study\n",
        "best_params = {'learning_rate': 0.0000860370374400373, 'weight_decay': 0.00008459884214639005, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3, 'lr_scheduler_type': 'polynomial'} # Manually typed the best_params for future use\n",
        "name_path = \"/best_model_roberta_base_tweet_rec5\"\n",
        "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
        "\n",
        "# Training the Model (1), using Optuna-study's best trial HPs - RoBERTa-Base-Tweet:\n",
        "train_model_with_hyperparams_HF(architecture=\"twitter-roberta-base\", best_params=best_params,save_path=save_path)\n",
        "\n",
        "# Zip the whole model folder\n",
        "shutil.make_archive(save_path, \"zip\", save_path)\n",
        "\n",
        "# Download the zip to your computer\n",
        "files.download(f\"{save_path}.zip\")"
      ],
      "metadata": {
        "id": "TuvWwi3tio7F"
      },
      "id": "TuvWwi3tio7F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **BerTweet-Base:**"
      ],
      "metadata": {
        "id": "OpdCqyAHiMf4"
      },
      "id": "OpdCqyAHiMf4"
    },
    {
      "cell_type": "code",
      "source": [
        "# best_params = study_bertweet_base_rec5.best_params  # get best HPs from the model's Optuna study\n",
        "best_params = {'learning_rate': 7.668855564109297e-05, 'weight_decay': 4.8978169582912055e-06, 'patience': 9, 'batch_size': 64, 'num_layers_finetune': 3, 'lr_scheduler_type': 'linear'} # Manually typed the best_params for future use\n",
        "name_path = \"/best_model_bertweet_base_rec5\"\n",
        "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
        "\n",
        "# Training the Model (2), using Optuna-study's best trial HPs - BERTweet-Base:\n",
        "train_model_with_hyperparams_HF(architecture=\"bertweet-base\", best_params=best_params,save_path=save_path)\n",
        "\n",
        "# Zip the whole model folder\n",
        "shutil.make_archive(save_path, \"zip\", save_path)\n",
        "\n",
        "# Download the zip to your computer\n",
        "files.download(f\"{save_path}.zip\")"
      ],
      "metadata": {
        "id": "MaWtswBPTW6h"
      },
      "id": "MaWtswBPTW6h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compression Techniques**"
      ],
      "metadata": {
        "id": "hzSP936PdzI-"
      },
      "id": "hzSP936PdzI-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Technique (1) - Quantization**"
      ],
      "metadata": {
        "id": "4PQZhYHHfmpq"
      },
      "id": "4PQZhYHHfmpq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a model compression technique, **Quantization reduces model size and speeds up inference by converting weights to lower precision, quantizing them**. The function below applies **dynamic quantization (Post-Training) on a fine-tuned HF model** (which has gone through the final training phases above), evaluates it on the test set, and saves the quantized version."
      ],
      "metadata": {
        "id": "fn5RuBB9dwBE"
      },
      "id": "fn5RuBB9dwBE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Critical roots\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "model_root   = f\"{project_root}/Model_Weights\"\n",
        "\n",
        "# Define quant_root inside the project, for all quantized weights\n",
        "quant_root = f\"{project_root}/Quantized_Model_Weights\""
      ],
      "metadata": {
        "id": "bdXuo4Ti2-XU"
      },
      "id": "bdXuo4Ti2-XU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function which evaluates model performance given a specific dataset (loader) - train / test:\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Load relevant metrics - Accuracy, F1-Score, Precision & Recall:\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
        "        \"f1\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
        "        \"precision\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
        "        \"recall\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Vcw7tahMkkvz"
      },
      "id": "Vcw7tahMkkvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function quantizes a fine-tuned HF model, evaluates it on training & test sets, compares its performance with the previous model's, and saves the quantized version.\n",
        "def quantize_evaluate_and_compare(model_name, model_name_dir, best_params):\n",
        "\n",
        "  # Define original model path (trained weights) and quantized save path\n",
        "    model_path     = f\"{model_root}/{model_name_dir}\"\n",
        "    quantized_path = f\"{quant_root}/{model_name_dir}_quantized\"\n",
        "\n",
        "    # Select correct pretokenized dataset\n",
        "    if \"roberta\" in model_name_dir.lower():\n",
        "        pretokenized_dir = \"data/tokenized_twitter_roberta_base\" # the folder for saving the model\n",
        "    else:\n",
        "        pretokenized_dir = \"data/tokenized_bertweet_base\" # the folder for saving the model\n",
        "\n",
        "    # Load model & tokenizer - Initially on GPU but need to be moved to CPU before Quantization!\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # Merge train + validation for final training\n",
        "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
        "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
        "\n",
        "    # initialize loaders (train & test) from the pretokenized HF dataset\n",
        "    train_loader = DataLoader(\n",
        "        full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        ds[\"test\"], batch_size=min(2*best_params[\"batch_size\"], 128), shuffle=False,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Evaluate the performance of original model - first:\n",
        "    train_original = evaluate_model(model, train_loader, device)\n",
        "    test_original = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    # move original model to CPU for quantization post-evaluation\n",
        "    model = model.to(\"cpu\")\n",
        "\n",
        "    # Apply dynamic quantization\n",
        "    quantized_model = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "    ).to(\"cpu\")\n",
        "\n",
        "    # Save quantized model's config & weights\n",
        "    os.makedirs(quantized_path, exist_ok=True) # Create directory if it doesn't exist\n",
        "    torch.save(quantized_model.state_dict(), os.path.join(quantized_path, \"pytorch_model.bin\"))\n",
        "    tokenizer.save_pretrained(quantized_path)\n",
        "\n",
        "    # Evaluate the performance of quantized model - second:\n",
        "    train_quantized = evaluate_model(quantized_model, train_loader, torch.device(\"cpu\"))\n",
        "    test_quantized = evaluate_model(quantized_model, test_loader, torch.device(\"cpu\"))\n",
        "\n",
        "    # Count number of parameters in both models - original & quantized:\n",
        "    original_params = sum(p.numel() for p in model.parameters())\n",
        "    quantized_params = sum(p.numel() for p in quantized_model.parameters())\n",
        "\n",
        "    # Collect results into a DataFrame\n",
        "    results = pd.DataFrame([{\n",
        "        \"original_params\": original_params,\n",
        "        \"quantized_params\": quantized_params,\n",
        "        \"param_reduction\": original_params - quantized_params,\n",
        "        \"param_ratio\": quantized_params / original_params,\n",
        "        # Accuracy\n",
        "        \"train_accuracy_original\": train_original[\"accuracy\"],\n",
        "        \"test_accuracy_original\": test_original[\"accuracy\"],\n",
        "        \"train_accuracy_quantized\": train_quantized[\"accuracy\"],\n",
        "        \"test_accuracy_quantized\": test_quantized[\"accuracy\"],\n",
        "        \"train_accuracy_drop\": train_original[\"accuracy\"] - train_quantized[\"accuracy\"],\n",
        "        \"test_accuracy_drop\": test_original[\"accuracy\"] - test_quantized[\"accuracy\"],\n",
        "\n",
        "        # F1-Score\n",
        "        \"train_f1_original\": train_original[\"f1\"],\n",
        "        \"test_f1_original\": test_original[\"f1\"],\n",
        "        \"train_f1_quantized\": train_quantized[\"f1\"],\n",
        "        \"test_f1_quantized\": test_quantized[\"f1\"],\n",
        "        \"train_f1_drop\": train_original[\"f1\"] - train_quantized[\"f1\"],\n",
        "        \"test_f1_drop\": test_original[\"f1\"] - test_quantized[\"f1\"],\n",
        "\n",
        "        # Precision\n",
        "        \"train_precision_original\": train_original[\"precision\"],\n",
        "        \"test_precision_original\": test_original[\"precision\"],\n",
        "        \"train_precision_quantized\": train_quantized[\"precision\"],\n",
        "        \"test_precision_quantized\": test_quantized[\"precision\"],\n",
        "        \"train_precision_drop\": train_original[\"precision\"] - train_quantized[\"precision\"],\n",
        "        \"test_precision_drop\": test_original[\"precision\"] - test_quantized[\"precision\"],\n",
        "\n",
        "        # Recall\n",
        "        \"train_recall_original\": train_original[\"recall\"],\n",
        "        \"test_recall_original\": test_original[\"recall\"],\n",
        "        \"train_recall_quantized\": train_quantized[\"recall\"],\n",
        "        \"test_recall_quantized\": test_quantized[\"recall\"],\n",
        "        \"train_recall_drop\": train_original[\"recall\"] - train_quantized[\"recall\"],\n",
        "        \"test_recall_drop\": test_original[\"recall\"] - test_quantized[\"recall\"],\n",
        "    }], index=[model_name])\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "jpNx70hGdh9C"
      },
      "id": "jpNx70hGdh9C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantizing all 4 models with their corresponding batch sizes (typed manually!)\n",
        "model_configs = {\n",
        "    \"BERTweet-Base (rec4)\": (\"best_model_bertweet_base_rec4\", 128),\n",
        "    \"BERTweet-Base (rec5 - HF)\": (\"best_model_bertweet_base_rec5\", 64),\n",
        "    \"RoBERTa-Base-Tweet (rec4)\": (\"best_model_roberta_base_tweet_rec4\", 128),\n",
        "    \"RoBERTa-Base-Tweet (rec5 - HF)\": (\"best_model_roberta_base_tweet_rec5\", 128)\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model_name, (model_name_dir, batch_size) in model_configs.items():\n",
        "    print(f\"\\nPost-Training Quantization Results for {model_name}:\")\n",
        "    results_df = quantize_evaluate_and_compare(model_name, model_name_dir, {\"batch_size\": batch_size})\n",
        "    results_df.index.name = \"model_name\"\n",
        "    all_results.append(results_df)\n",
        "    display(results_df)\n",
        "\n",
        "# Concatenate into one DataFrame\n",
        "all_results_df = pd.concat(all_results, ignore_index=False)"
      ],
      "metadata": {
        "id": "1PT6x1vjJaBL"
      },
      "id": "1PT6x1vjJaBL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display quantization results over all 4 models\n",
        "display(all_results_df)\n",
        "\n",
        "# Save for future use\n",
        "save_path = f\"{quant_root}/quantization_results.csv\"\n",
        "all_results_df.to_csv(save_path, index=True)\n",
        "print(f\"\\nAll post-training quantization results saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "buYaEHN4JcZE"
      },
      "id": "buYaEHN4JcZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Technique (2) - Pruning**"
      ],
      "metadata": {
        "id": "S-zqP-3fY_Gz"
      },
      "id": "S-zqP-3fY_Gz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a model compression technique, **Pruning reduces model size and speeds up inference by setting \"unimportant\" weights to 0.**. In this project, we proceeded implementing **Unstructured global Pruning - setting a portion** (40% by default) **of trained model weights with the smallest magnitudes (in absolute values) to 0**. The function below applies **globally (on ALL LINEAR / ALL LAYERS**, depending on user need), **on a fine-tuned HF model** (which has gone through the final training phases above), i.e. it looks for the portion of weights with the smallest magnitudes (in absolute values) and prunes them - sets them to 0. It then evaluates the pruned model on the test set, and saves the quantized version."
      ],
      "metadata": {
        "id": "lawbk95SZGLL"
      },
      "id": "lawbk95SZGLL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Critical roots\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "model_root   = f\"{project_root}/Model_Weights\"\n",
        "\n",
        "# Define pruned_root inside the project, for all pruned weights\n",
        "prune_root = f\"{project_root}/Pruned_Model_Weights\""
      ],
      "metadata": {
        "id": "ek37jtu2_PYH"
      },
      "id": "ek37jtu2_PYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function prunes a fine-tuned HF model, evaluates it on training & test sets, compares its performance with the previous model's, and saves the pruned version.\n",
        "# is_linear = a boolean variable set by the user whether global unstructured pruning of is desired only across the linear layers, or across all model weights. False by default.\n",
        "def prune_evaluate_and_compare(model_name, model_name_dir, best_params, is_linear = False):\n",
        "\n",
        "  # Define original model path (trained weights) and pruned save path\n",
        "    model_path     = f\"{model_root}/{model_name_dir}\"\n",
        "    pruned_path = f\"{prune_root}/{model_name_dir}_pruned\"\n",
        "\n",
        "    # Select correct pretokenized dataset\n",
        "    if \"roberta\" in model_name_dir.lower():\n",
        "        pretokenized_dir = \"data/tokenized_twitter_roberta_base\" # the folder for saving the model\n",
        "    else:\n",
        "        pretokenized_dir = \"data/tokenized_bertweet_base\" # the folder for saving the model\n",
        "\n",
        "    # Load model & tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir) # Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # Merge train + validation for final training\n",
        "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
        "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
        "\n",
        "    # initialize loaders (train & test) from the pretokenized HF dataset\n",
        "    train_loader = DataLoader(\n",
        "        full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "        persistent_workers=True, prefetch_factor=2\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        ds[\"test\"], batch_size=min(2*best_params[\"batch_size\"], 128), shuffle=False,\n",
        "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Evaluate the performance of original model - first:\n",
        "    train_original = evaluate_model(model, train_loader, device)\n",
        "    test_original = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    # Define the pruned model by reloading the original model\n",
        "    pruned_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "\n",
        "    # Collect layers to be pruned:\n",
        "    if is_linear:\n",
        "      parameters_to_prune = [(m, \"weight\") for m in pruned_model.modules() if isinstance(m, nn.Linear)] # if is_linear == True -> Unstructured-global-pruning only across linear layers\n",
        "    else:\n",
        "      parameters_to_prune = [(m, \"weight\") for m in pruned_model.modules() if hasattr(m, \"weight\")] # otherwise -> Unstructured-global-pruning only across linear layers\n",
        "\n",
        "    # Apply pruning (global, unstructured)\n",
        "    prune.global_unstructured(\n",
        "        parameters_to_prune,\n",
        "        pruning_method=prune.L1Unstructured,\n",
        "        amount=0.4 # set by default. Essentially, 40% of parameters of model's (linear / all, user-dependent as noted above) layers with the smallest magnitudes (absolute values) would be pruned - set to 0.\n",
        "    )\n",
        "\n",
        "    for m, n in parameters_to_prune:\n",
        "      prune.remove(m, n) # removing pruned weights (already set to 0) from the pruned model, to observe the actual parameter reduction.\n",
        "\n",
        "    # Save pruned model's config & weights\n",
        "    os.makedirs(pruned_path, exist_ok=True) # Create directory if it doesn't exist\n",
        "    pruned_model.save_pretrained(pruned_path)\n",
        "    # torch.save(pruned_model.state_dict(), os.path.join(pruned_path, \"pytorch_model.bin\"))\n",
        "    tokenizer.save_pretrained(pruned_path)\n",
        "\n",
        "    # Evaluate the performance of pruned model - second:\n",
        "    train_pruned = evaluate_model(pruned_model, train_loader, device)\n",
        "    test_pruned = evaluate_model(pruned_model, test_loader, device)\n",
        "\n",
        "    # Count number of parameters in both models - original & pruned:\n",
        "    original_params = sum(p.numel() for p in model.parameters())\n",
        "    pruned_params = sum(torch.count_nonzero(p).item() for p in pruned_model.parameters())\n",
        "\n",
        "    # Collect results into a DataFrame\n",
        "    results = pd.DataFrame([{\n",
        "        \"original_params\": original_params,\n",
        "        \"pruned_params\": pruned_params,\n",
        "        \"param_reduction\": original_params - pruned_params,\n",
        "        \"param_ratio\": pruned_params / original_params,\n",
        "        # Accuracy\n",
        "        \"train_accuracy_original\": train_original[\"accuracy\"],\n",
        "        \"test_accuracy_original\": test_original[\"accuracy\"],\n",
        "        \"train_accuracy_pruned\": train_pruned[\"accuracy\"],\n",
        "        \"test_accuracy_pruned\": test_pruned[\"accuracy\"],\n",
        "        \"train_accuracy_drop\": train_original[\"accuracy\"] - train_pruned[\"accuracy\"],\n",
        "        \"test_accuracy_drop\": test_original[\"accuracy\"] - test_pruned[\"accuracy\"],\n",
        "\n",
        "        # F1-Score\n",
        "        \"train_f1_original\": train_original[\"f1\"],\n",
        "        \"test_f1_original\": test_original[\"f1\"],\n",
        "        \"train_f1_pruned\": train_pruned[\"f1\"],\n",
        "        \"test_f1_pruned\": test_pruned[\"f1\"],\n",
        "        \"train_f1_drop\": train_original[\"f1\"] - train_pruned[\"f1\"],\n",
        "        \"test_f1_drop\": test_original[\"f1\"] - test_pruned[\"f1\"],\n",
        "\n",
        "        # Precision\n",
        "        \"train_precision_original\": train_original[\"precision\"],\n",
        "        \"test_precision_original\": test_original[\"precision\"],\n",
        "        \"train_precision_pruned\": train_pruned[\"precision\"],\n",
        "        \"test_precision_pruned\": test_pruned[\"precision\"],\n",
        "        \"train_precision_drop\": train_original[\"precision\"] - train_pruned[\"precision\"],\n",
        "        \"test_precision_drop\": test_original[\"precision\"] - test_pruned[\"precision\"],\n",
        "\n",
        "        # Recall\n",
        "        \"train_recall_original\": train_original[\"recall\"],\n",
        "        \"test_recall_original\": test_original[\"recall\"],\n",
        "        \"train_recall_pruned\": train_pruned[\"recall\"],\n",
        "        \"test_recall_pruned\": test_pruned[\"recall\"],\n",
        "        \"train_recall_drop\": train_original[\"recall\"] - train_pruned[\"recall\"],\n",
        "        \"test_recall_drop\": test_original[\"recall\"] - test_pruned[\"recall\"],\n",
        "    }], index=[model_name])\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "G83_eLUmAB9W"
      },
      "id": "G83_eLUmAB9W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruning all 4 models with their corresponding batch sizes (typed manually!), considering ONLY LINEAR layers\n",
        "model_configs = {\n",
        "    \"BERTweet-Base (rec4)\": (\"best_model_bertweet_base_rec4\", 128),\n",
        "    \"BERTweet-Base (rec5 - HF)\": (\"best_model_bertweet_base_rec5\", 64),\n",
        "    \"RoBERTa-Base-Tweet (rec4)\": (\"best_model_roberta_base_tweet_rec4\", 128),\n",
        "    \"RoBERTa-Base-Tweet (rec5 - HF)\": (\"best_model_roberta_base_tweet_rec5\", 128)\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model_name, (model_name_dir, batch_size) in model_configs.items():\n",
        "    print(f\"\\nUnstructured global Pruning Results for {model_name} (considering linear layers only):\")\n",
        "    results_df = prune_evaluate_and_compare(model_name, model_name_dir, {\"batch_size\": batch_size}, is_linear = True)\n",
        "    results_df.index.name = \"model_name\"\n",
        "    all_results.append(results_df)\n",
        "    display(results_df)\n",
        "\n",
        "# Concatenate into one DataFrame\n",
        "all_results_df = pd.concat(all_results, ignore_index=False)"
      ],
      "metadata": {
        "id": "rFyZBYaV_Og3"
      },
      "id": "rFyZBYaV_Og3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display pruning results over all 4 models - \"LINEAR CASE\"\n",
        "display(all_results_df)\n",
        "\n",
        "# Save for future use\n",
        "save_path = f\"{prune_root}/pruning_results_linear.csv\"\n",
        "all_results_df.to_csv(save_path, index=True)\n",
        "print(f\"\\nAll unstructured global pruning results (considering linear layers only) saved to: {save_path} \")"
      ],
      "metadata": {
        "id": "hJONIVOKlZ5B"
      },
      "id": "hJONIVOKlZ5B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruning all 4 models with their corresponding batch sizes (typed manually!), considering ALL model layers\n",
        "all_results = []\n",
        "\n",
        "for model_name, (model_name_dir, batch_size) in model_configs.items():\n",
        "    print(f\"\\nUnstructured global Pruning Results for {model_name} (considering all model layers):\")\n",
        "    results_df = prune_evaluate_and_compare(model_name, model_name_dir, {\"batch_size\": batch_size}, is_linear = False)\n",
        "    results_df.index.name = \"model_name\"\n",
        "    all_results.append(results_df)\n",
        "    display(results_df)\n",
        "\n",
        "# Concatenate into one DataFrame\n",
        "all_results_df = pd.concat(all_results, ignore_index=False)"
      ],
      "metadata": {
        "id": "ZkyQJTEjkWWC"
      },
      "id": "ZkyQJTEjkWWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display pruning results over all 4 models - \"GENERALIZED CASE\"\n",
        "display(all_results_df)\n",
        "\n",
        "# Save for future use\n",
        "save_path = f\"{prune_root}/pruning_results_generalized.csv\"\n",
        "all_results_df.to_csv(save_path, index=True)\n",
        "print(f\"\\nAll unstructured global pruning results (considering all model layers) saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "_wGv-j6u_Zu7"
      },
      "id": "_wGv-j6u_Zu7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Technique (3) - Knowledge-Distillation (KD)**"
      ],
      "metadata": {
        "id": "5KJ5vpenIwKR"
      },
      "id": "5KJ5vpenIwKR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a compression technique, **Knowledge Distillation** reduces model size by **training a compact student model** (compact = with much less parameters) **to imitate a stronger teacher model by matching the teacher’s soft predictions while still learning from the gold labels**. In our pipeline the fine-tuned teacher from Model_Weights is frozen and evaluated in eval() mode, and the student (e.g., arampacha/roberta-tiny) is optimized with the mixed objective α·CE(y, s) + (1−α)·T²·KL(softmax(t/T) || log_softmax(s/T)), where T is the temperature and α controls the balance between label supervision and teacher guidance. We train on the pre-tokenized datasets, log train and test metrics for both teacher and student to Weights & Biases, and save the best student checkpoint under KD_Model_Weights, along with a CSV of results for later comparison."
      ],
      "metadata": {
        "id": "MM2928PKIw28"
      },
      "id": "MM2928PKIw28"
    },
    {
      "cell_type": "code",
      "source": [
        "# Critical roots\n",
        "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
        "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
        "model_root   = f\"{project_root}/Model_Weights\"\n",
        "\n",
        "# Define KD_root inside the project, for all KD weights\n",
        "KD_root = f\"{project_root}/KD_Model_Weights\""
      ],
      "metadata": {
        "id": "DK3kk8cxIMrs"
      },
      "id": "DK3kk8cxIMrs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class TrainEvalCallback(TrainerCallback):\n",
        "    def __init__(self, trainer, train_dataset):\n",
        "        self.trainer = trainer\n",
        "        self.train_dataset = train_dataset\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        # HF already computed eval_* on the test set this epoch.\n",
        "        if metrics and wandb.run is not None:\n",
        "            out = {}\n",
        "            for k, v in metrics.items():\n",
        "                if isinstance(v, (int, float)):\n",
        "                    key = k[5:] if k.startswith(\"eval_\") else k\n",
        "                    out[f\"test/student_{key}\"] = float(v)\n",
        "            wandb.log(out)\n",
        "        return control\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        # Compute train metrics once per epoch\n",
        "        train_metrics = self.trainer.evaluate(\n",
        "            eval_dataset=self.train_dataset,\n",
        "            metric_key_prefix=\"train\"  # -> train_loss, train_accuracy, ...\n",
        "        )\n",
        "\n",
        "        # 1) HF log keeps the pretty table\n",
        "        self.trainer.log(train_metrics)\n",
        "\n",
        "        # 1a) ALSO put 'loss' so the \"Training Loss\" column isn't \"No log\"\n",
        "        if \"train_loss\" in train_metrics:\n",
        "            self.trainer.log({\"loss\": float(train_metrics[\"train_loss\"])})\n",
        "\n",
        "        # 2) W&B: log under train/student_* (no explicit step)\n",
        "        if wandb.run is not None:\n",
        "            out = {}\n",
        "            for k, v in train_metrics.items():\n",
        "                if isinstance(v, (int, float)):\n",
        "                    key = k[6:] if k.startswith(\"train_\") else k\n",
        "                    out[f\"train/student_{key}\"] = float(v)\n",
        "            wandb.log(out)\n",
        "        return control"
      ],
      "metadata": {
        "id": "xjeeIVRjjNqu"
      },
      "id": "xjeeIVRjjNqu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # freeze + eval teacher\n",
        "        self.teacher = teacher_model\n",
        "        self.teacher.eval()\n",
        "        for p in self.teacher.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.temperature = float(temperature)\n",
        "        self.alpha = float(alpha)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _teacher_forward(self, **inputs):\n",
        "        # teacher never sees labels\n",
        "        inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
        "        return self.teacher(**inputs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # 1) remove labels so student model doesn't compute internal CE\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        if labels.dtype != torch.long:\n",
        "            labels = labels.long()\n",
        "        # cheap guard; safe even if always 0..4\n",
        "        if torch.any(labels.lt(0)) or torch.any(labels.gt(4)):\n",
        "            labels = labels.clamp_(0, 4)\n",
        "\n",
        "        # 2) student forward (no labels)\n",
        "        outputs_student = model(**inputs)\n",
        "\n",
        "        # 3) teacher forward (eval, no grads)\n",
        "        with torch.no_grad():\n",
        "            outputs_teacher = self._teacher_forward(**inputs)\n",
        "\n",
        "        # 4) KD loss\n",
        "        t = self.temperature\n",
        "        loss_ce = F.cross_entropy(outputs_student.logits, labels)\n",
        "        loss_kl = F.kl_div(\n",
        "            F.log_softmax(outputs_student.logits / t, dim=-1),\n",
        "            F.softmax(outputs_teacher.logits / t, dim=-1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (t * t)\n",
        "\n",
        "        loss = self.alpha * loss_ce + (1.0 - self.alpha) * loss_kl\n",
        "        return (loss, outputs_student) if return_outputs else loss"
      ],
      "metadata": {
        "id": "bJYMIUb1z-bB"
      },
      "id": "bJYMIUb1z-bB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distill_evaluate_and_compare(model_name, model_name_dir, best_params, student_model_name, alpha=0.5, temperature=2.0, num_epochs=5):\n",
        "\n",
        "    # Paths\n",
        "    teacher_path = f\"{model_root}/{model_name_dir}\"\n",
        "    student_slug = student_model_name.replace(\"/\", \"-\")\n",
        "    KD_path      = f\"{KD_root}/{model_name_dir}_distilled_student_{student_slug}\"\n",
        "    os.makedirs(KD_path, exist_ok=True)\n",
        "\n",
        "    # Select correct pretokenized dataset\n",
        "    if \"roberta\" in model_name_dir.lower():\n",
        "        pretokenized_dir = \"data/tokenized_twitter_roberta_base\"\n",
        "    else:\n",
        "        pretokenized_dir = \"data/tokenized_bertweet_base\"\n",
        "\n",
        "    # safety: correct dtypes + torch output\n",
        "    ds = load_from_disk(pretokenized_dir)\n",
        "    for split in ds:\n",
        "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))\n",
        "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
        "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # Merge train + validation for final training\n",
        "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]]).shuffle(seed=42)\n",
        "    test_dataset = ds[\"test\"]\n",
        "\n",
        "    # Load Teacher + tokenizer\n",
        "    teacher = AutoModelForSequenceClassification.from_pretrained(teacher_path).to(device)\n",
        "    teacher.eval()\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad = False\n",
        "    tokenizer = AutoTokenizer.from_pretrained(teacher_path)\n",
        "\n",
        "    # Load Student + tokenizer (vocab aligned)\n",
        "    student = AutoModelForSequenceClassification.from_pretrained(student_model_name, num_labels=5, ignore_mismatched_sizes=True).to(device)\n",
        "    student_tokenizer = tokenizer\n",
        "    student.resize_token_embeddings(len(student_tokenizer))\n",
        "\n",
        "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
        "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "\n",
        "    # W&B init (project name includes teacher dir + student)\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "    wandb.init(\n",
        "        project=f\"KD_{model_name}__student_{student_slug}_21.8.2025\",\n",
        "        entity=\"idoshahar96-tel-aviv-university\",\n",
        "        config={\n",
        "            \"learning_rate\": best_params[\"learning_rate\"],\n",
        "            \"weight_decay\": best_params[\"weight_decay\"],\n",
        "            \"batch_size\": best_params[\"batch_size\"],\n",
        "            \"num_layers_finetune\": best_params.get(\"num_layers_finetune\", None),\n",
        "            \"teacher_path\": teacher_path,\n",
        "            \"student_model\": student_model_name,\n",
        "            \"alpha\": alpha,\n",
        "            \"temperature\": temperature,\n",
        "            \"epochs\": num_epochs,\n",
        "        },\n",
        "        name=f\"KD_{model_name}__{student_slug}\",\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # TrainingArguments for the Hugging Face Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=KD_path, # where checkpoints will be saved\n",
        "        per_device_train_batch_size=best_params[\"batch_size\"],\n",
        "        per_device_eval_batch_size=best_params[\"batch_size\"],\n",
        "        learning_rate=best_params[\"learning_rate\"],\n",
        "        weight_decay=best_params[\"weight_decay\"],\n",
        "        num_train_epochs=num_epochs, # Setting the number of epochs for training - 5\n",
        "        eval_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
        "        save_strategy=\"epoch\",        # save a checkpoint at the end of each epoch\n",
        "        logging_strategy=\"epoch\",     # log metrics at the end of each epoch\n",
        "        load_best_model_at_end=True,  # reload the best checkpoint (based on metric_for_best_model)\n",
        "        metric_for_best_model=\"accuracy\", # optimize w.r.t accuracy\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        remove_unused_columns=False,\n",
        "        label_names=[\"labels\"],\n",
        "        report_to=\"wandb\",\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer_distill = DistillationTrainer(\n",
        "        model=student,\n",
        "        teacher_model=teacher,\n",
        "        args=training_args,\n",
        "        train_dataset=full_train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        temperature=temperature,\n",
        "        alpha=alpha,\n",
        "    )\n",
        "\n",
        "    # per-epoch: TEST (from on_evaluate) then TRAIN (from on_epoch_end)\n",
        "    trainer_distill.add_callback(TrainEvalCallback(trainer_distill, full_train_dataset))\n",
        "\n",
        "    # TRAIN - KD\n",
        "    trainer_distill.train()\n",
        "    print(\"\\nDistillation complete. Student trained & best model loaded.\")\n",
        "\n",
        "    # Final metrics (TEST first, then TRAIN) for teacher & student\n",
        "    def _eval_with(model_to_eval, dataset, prefix):\n",
        "        tmp = Trainer(\n",
        "            model=model_to_eval,\n",
        "            args=training_args,\n",
        "            eval_dataset=dataset,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "        return tmp.evaluate(metric_key_prefix=prefix)\n",
        "\n",
        "    # Teacher metrics\n",
        "    teacher_test   = _eval_with(teacher, test_dataset,        \"teacher_test\")\n",
        "    teacher_train  = _eval_with(teacher, full_train_dataset, \"teacher_train\")\n",
        "\n",
        "    # Student metrics\n",
        "    student_test   = trainer_distill.evaluate(eval_dataset=test_dataset,        metric_key_prefix=\"student_test\")\n",
        "    student_train  = trainer_distill.evaluate(eval_dataset=full_train_dataset, metric_key_prefix=\"student_train\")\n",
        "\n",
        "    # Log to W&B (TEST first, then TRAIN) so panels order naturally\n",
        "    if wandb.run is not None:\n",
        "        def log_group(prefix, who, d):\n",
        "            out = {}\n",
        "            for k, v in d.items():\n",
        "                if isinstance(v, (int, float)):\n",
        "                    key = k.split(f\"{who}_\", 1)[-1] if f\"{who}_\" in k else k\n",
        "                    out[f\"{prefix}/{who}_{key}\"] = float(v)\n",
        "            wandb.log(out)\n",
        "\n",
        "        log_group(\"test\",  \"teacher\", teacher_test)\n",
        "        log_group(\"test\",  \"student\", student_test)\n",
        "        log_group(\"train\", \"teacher\", teacher_train)\n",
        "        log_group(\"train\", \"student\", student_train)\n",
        "\n",
        "    # Count params & save student model (best-epoch)\n",
        "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
        "    student_params = sum(p.numel() for p in student.parameters())\n",
        "    print(f\"Teacher params: {teacher_params:,}\")\n",
        "    print(f\"Student params: {student_params:,}\")\n",
        "    trainer_distill.model.save_pretrained(KD_path)\n",
        "    student_tokenizer.save_pretrained(KD_path)\n",
        "    print(f\"Best student model saved to {KD_path}\")\n",
        "    wandb.finish()\n",
        "\n",
        "    # Build output DataFrame (params statistics, then metrics, then drops in metrics while comparing the models)\n",
        "    def g(d, k):\n",
        "        v = d.get(k, float(\"nan\"))\n",
        "        try:\n",
        "            return float(v)\n",
        "        except Exception:\n",
        "            return float(\"nan\")\n",
        "\n",
        "    def pack(metric_name, t_train, t_test, s_train, s_test):\n",
        "        tt = g(t_train, f\"teacher_train_{metric_name}\")\n",
        "        te = g(t_test,  f\"teacher_test_{metric_name}\")\n",
        "        st = g(s_train, f\"student_train_{metric_name}\")\n",
        "        se = g(s_test,  f\"student_test_{metric_name}\")\n",
        "        return {\n",
        "            f\"teacher_train_{metric_name}\": tt,\n",
        "            f\"teacher_test_{metric_name}\":  te,\n",
        "            f\"student_train_{metric_name}\": st,\n",
        "            f\"student_test_{metric_name}\":  se,\n",
        "            f\"drop_train_{metric_name}\":    tt - st,  # teacher − student\n",
        "            f\"drop_test_{metric_name}\":     te - se,\n",
        "        }\n",
        "\n",
        "    row = {\n",
        "        \"teacher_params\": teacher_params,\n",
        "        \"student_params\": student_params,\n",
        "        \"param_reduction\": teacher_params - student_params,\n",
        "        \"param_ratio\": student_params / teacher_params,\n",
        "    }\n",
        "    for m in [\"accuracy\", \"f1\", \"precision\", \"recall\"]:\n",
        "        row.update(pack(m, teacher_train, teacher_test, student_train, student_test))\n",
        "\n",
        "    results = pd.DataFrame([row], index=[model_name])\n",
        "    return results"
      ],
      "metadata": {
        "id": "iW3jIsX_wPvU"
      },
      "id": "iW3jIsX_wPvU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Knowledge-Distillation (KD) over all 4 models with their corresponding best-params (typed manually!)\n",
        "model_configs = {\n",
        "    \"BERTweet-Base (rec4)\": (\"best_model_bertweet_base_rec4\", {'learning_rate': 0.0001184412471705182, 'weight_decay': 1.2699696348040995e-05, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3}),\n",
        "    \"BERTweet-Base (rec5 - HF)\": (\"best_model_bertweet_base_rec5\", {'learning_rate': 7.668855564109297e-05, 'weight_decay': 4.8978169582912055e-06, 'patience': 9, 'batch_size': 64, 'num_layers_finetune': 3, 'lr_scheduler_type': 'linear'}),\n",
        "    \"RoBERTa-Base-Tweet (rec4)\": (\"best_model_roberta_base_tweet_rec4\", {'learning_rate': 0.0003834791389042033, 'weight_decay': 2.88286253103848e-06, 'patience': 7, 'batch_size': 128, 'num_layers_finetune': 3}),\n",
        "    \"RoBERTa-Base-Tweet (rec5 - HF)\": (\"best_model_roberta_base_tweet_rec5\", {'learning_rate': 0.0000860370374400373, 'weight_decay': 0.00008459884214639005, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3, 'lr_scheduler_type': 'polynomial'})\n",
        "}\n",
        "\n",
        "all_results = []\n",
        "student_model_name = \"arampacha/roberta-tiny\" # Example student model - BERT-Tiny (truly)\n",
        "\n",
        "for model_name, (model_name_dir, best_params) in model_configs.items():\n",
        "    print(f\"\\nKnowledge-Distillation (KD) Results for TEACHER: {model_name}, STUDENT: {student_model_name} (5 epochs):\")\n",
        "    results_df = distill_evaluate_and_compare(model_name, model_name_dir, best_params, student_model_name=student_model_name)\n",
        "    results_df.index.name = \"model_name\"\n",
        "    all_results.append(results_df)\n",
        "    display(results_df)\n",
        "\n",
        "# Concatenate into one DataFrame\n",
        "all_results_df = pd.concat(all_results, ignore_index=False)"
      ],
      "metadata": {
        "id": "R2bpF5Z5JDu9"
      },
      "id": "R2bpF5Z5JDu9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add student name column (redundant because the student model name would be clear from the CSV file directory, but we wanted to make the results even clearer, by displaying the student model name even more explicitly)\n",
        "all_results_df[\"student_model_name\"] = \"arampacha-roberta-tiny\"\n",
        "\n",
        "# Reorder so \"student_model_name\" is right after the index\n",
        "cols = all_results_df.columns.tolist()\n",
        "cols = [\"student_model_name\"] + [c for c in cols if c != \"student_model_name\"]\n",
        "all_results_df = all_results_df[cols]\n",
        "all_results_df[\"student_model_name\"] = \"arampacha-roberta-tiny\"\n",
        "# Display Knowledge-Distillation (KD) results over all 4 models\n",
        "display(all_results_df)\n",
        "\n",
        "student_model_name_for_csv = \"arampacha_roberta_tiny\"\n",
        "\n",
        "# Save for future use\n",
        "save_path = f\"{KD_root}/KD_results_{student_model_name_for_csv}.csv\"\n",
        "all_results_df.to_csv(save_path, index=True)\n",
        "print(f\"\\nAll Knowledge-Distillation (KD) results saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "QULDcH4GJUIK"
      },
      "id": "QULDcH4GJUIK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}