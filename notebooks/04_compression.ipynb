{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713c48a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IdanKanat/COVID_NLP_Advanced_DL_Project/blob/main/AdvancedTopicsDL_Project_IdanKanat%26IdoShahar_COVID_NLP_21.8.2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27337f0-1b88-4650-9e92-fd3d5517f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: optuna in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: wandb in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.34.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2025.7.14)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35976\\1376978527.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./01_EDA_and_data_preprocessing.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./02_finetune_WITHOUT_HF_Trainer.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./03_finetune_with_HF_Trainer.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:741\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    740\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 741\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39msafe_execfile_ipy(filename, raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3005\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   3003\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 3005\u001b[0m     result\u001b[38;5;241m.\u001b[39mraise_error()\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   3007\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35976\\1376978527.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "%run ./01_EDA_and_data_preprocessing.ipynb\n",
    "%run ./02_finetune_WITHOUT_HF_Trainer.ipynb\n",
    "%run ./03_finetune_with_HF_Trainer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hzSP936PdzI-",
   "metadata": {
    "id": "hzSP936PdzI-"
   },
   "source": [
    "# **Compression Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4PQZhYHHfmpq",
   "metadata": {
    "id": "4PQZhYHHfmpq"
   },
   "source": [
    "## **Technique (1) - Quantization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fn5RuBB9dwBE",
   "metadata": {
    "id": "fn5RuBB9dwBE"
   },
   "source": [
    "As a model compression technique, **Quantization reduces model size and speeds up inference by converting weights to lower precision, quantizing them**. The function below applies **dynamic quantization (Post-Training) on a fine-tuned HF model** (which has gone through the final training phases above), evaluates it on the test set, and saves the quantized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdXuo4Ti2-XU",
   "metadata": {
    "id": "bdXuo4Ti2-XU"
   },
   "outputs": [],
   "source": [
    "# Critical roots\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "model_root   = f\"{project_root}/Model_Weights\"\n",
    "\n",
    "# Define quant_root inside the project, for all quantized weights\n",
    "quant_root = f\"{project_root}/Quantized_Model_Weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vcw7tahMkkvz",
   "metadata": {
    "id": "Vcw7tahMkkvz"
   },
   "outputs": [],
   "source": [
    "# Helper function which evaluates model performance given a specific dataset (loader) - train / test:\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Load relevant metrics - Accuracy, F1-Score, Precision & Recall:\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"f1\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"precision\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jpNx70hGdh9C",
   "metadata": {
    "id": "jpNx70hGdh9C"
   },
   "outputs": [],
   "source": [
    "# This function quantizes a fine-tuned HF model, evaluates it on training & test sets, compares its performance with the previous model's, and saves the quantized version.\n",
    "def quantize_evaluate_and_compare(model_name, model_name_dir, best_params):\n",
    "\n",
    "  # Define original model path (trained weights) and quantized save path\n",
    "    model_path     = f\"{model_root}/{model_name_dir}\"\n",
    "    quantized_path = f\"{quant_root}/{model_name_dir}_quantized\"\n",
    "\n",
    "    # Select correct pretokenized dataset\n",
    "    if \"roberta\" in model_name_dir.lower():\n",
    "        pretokenized_dir = \"data/tokenized_twitter_roberta_base\" # the folder for saving the model\n",
    "    else:\n",
    "        pretokenized_dir = \"data/tokenized_bertweet_base\" # the folder for saving the model\n",
    "\n",
    "    # Load model & tokenizer - Initially on GPU but need to be moved to CPU before Quantization!\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # Merge train + validation for final training\n",
    "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
    "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
    "\n",
    "    # initialize loaders (train & test) from the pretokenized HF dataset\n",
    "    train_loader = DataLoader(\n",
    "        full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=2\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        ds[\"test\"], batch_size=min(2*best_params[\"batch_size\"], 128), shuffle=False,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate the performance of original model - first:\n",
    "    train_original = evaluate_model(model, train_loader, device)\n",
    "    test_original = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # move original model to CPU for quantization post-evaluation\n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Apply dynamic quantization\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    # Save quantized model's config & weights\n",
    "    os.makedirs(quantized_path, exist_ok=True) # Create directory if it doesn't exist\n",
    "    torch.save(quantized_model.state_dict(), os.path.join(quantized_path, \"pytorch_model.bin\"))\n",
    "    tokenizer.save_pretrained(quantized_path)\n",
    "\n",
    "    # Evaluate the performance of quantized model - second:\n",
    "    train_quantized = evaluate_model(quantized_model, train_loader, torch.device(\"cpu\"))\n",
    "    test_quantized = evaluate_model(quantized_model, test_loader, torch.device(\"cpu\"))\n",
    "\n",
    "    # Count number of parameters in both models - original & quantized:\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    quantized_params = sum(p.numel() for p in quantized_model.parameters())\n",
    "\n",
    "    # Collect results into a DataFrame\n",
    "    results = pd.DataFrame([{\n",
    "        \"original_params\": original_params,\n",
    "        \"quantized_params\": quantized_params,\n",
    "        \"param_reduction\": original_params - quantized_params,\n",
    "        \"param_ratio\": quantized_params / original_params,\n",
    "        # Accuracy\n",
    "        \"train_accuracy_original\": train_original[\"accuracy\"],\n",
    "        \"test_accuracy_original\": test_original[\"accuracy\"],\n",
    "        \"train_accuracy_quantized\": train_quantized[\"accuracy\"],\n",
    "        \"test_accuracy_quantized\": test_quantized[\"accuracy\"],\n",
    "        \"train_accuracy_drop\": train_original[\"accuracy\"] - train_quantized[\"accuracy\"],\n",
    "        \"test_accuracy_drop\": test_original[\"accuracy\"] - test_quantized[\"accuracy\"],\n",
    "\n",
    "        # F1-Score\n",
    "        \"train_f1_original\": train_original[\"f1\"],\n",
    "        \"test_f1_original\": test_original[\"f1\"],\n",
    "        \"train_f1_quantized\": train_quantized[\"f1\"],\n",
    "        \"test_f1_quantized\": test_quantized[\"f1\"],\n",
    "        \"train_f1_drop\": train_original[\"f1\"] - train_quantized[\"f1\"],\n",
    "        \"test_f1_drop\": test_original[\"f1\"] - test_quantized[\"f1\"],\n",
    "\n",
    "        # Precision\n",
    "        \"train_precision_original\": train_original[\"precision\"],\n",
    "        \"test_precision_original\": test_original[\"precision\"],\n",
    "        \"train_precision_quantized\": train_quantized[\"precision\"],\n",
    "        \"test_precision_quantized\": test_quantized[\"precision\"],\n",
    "        \"train_precision_drop\": train_original[\"precision\"] - train_quantized[\"precision\"],\n",
    "        \"test_precision_drop\": test_original[\"precision\"] - test_quantized[\"precision\"],\n",
    "\n",
    "        # Recall\n",
    "        \"train_recall_original\": train_original[\"recall\"],\n",
    "        \"test_recall_original\": test_original[\"recall\"],\n",
    "        \"train_recall_quantized\": train_quantized[\"recall\"],\n",
    "        \"test_recall_quantized\": test_quantized[\"recall\"],\n",
    "        \"train_recall_drop\": train_original[\"recall\"] - train_quantized[\"recall\"],\n",
    "        \"test_recall_drop\": test_original[\"recall\"] - test_quantized[\"recall\"],\n",
    "    }], index=[model_name])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1PT6x1vjJaBL",
   "metadata": {
    "id": "1PT6x1vjJaBL"
   },
   "outputs": [],
   "source": [
    "# Quantizing all 4 models with their corresponding batch sizes (typed manually!)\n",
    "model_configs = {\n",
    "    \"BERTweet-Base (rec4)\": (\"best_model_bertweet_base_rec4\", 128),\n",
    "    \"BERTweet-Base (rec5 - HF)\": (\"best_model_bertweet_base_rec5\", 64),\n",
    "    \"RoBERTa-Base-Tweet (rec4)\": (\"best_model_roberta_base_tweet_rec4\", 128),\n",
    "    \"RoBERTa-Base-Tweet (rec5 - HF)\": (\"best_model_roberta_base_tweet_rec5\", 128)\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name, (model_name_dir, batch_size) in model_configs.items():\n",
    "    print(f\"\\nPost-Training Quantization Results for {model_name}:\")\n",
    "    results_df = quantize_evaluate_and_compare(model_name, model_name_dir, {\"batch_size\": batch_size})\n",
    "    results_df.index.name = \"model_name\"\n",
    "    all_results.append(results_df)\n",
    "    display(results_df)\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "all_results_df = pd.concat(all_results, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buYaEHN4JcZE",
   "metadata": {
    "id": "buYaEHN4JcZE"
   },
   "outputs": [],
   "source": [
    "# Display quantization results over all 4 models\n",
    "display(all_results_df)\n",
    "\n",
    "# Save for future use\n",
    "save_path = f\"{quant_root}/quantization_results.csv\"\n",
    "all_results_df.to_csv(save_path, index=True)\n",
    "print(f\"\\nAll post-training quantization results saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S-zqP-3fY_Gz",
   "metadata": {
    "id": "S-zqP-3fY_Gz"
   },
   "source": [
    "## **Technique (2) - Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lawbk95SZGLL",
   "metadata": {
    "id": "lawbk95SZGLL"
   },
   "source": [
    "As a model compression technique, **Pruning reduces model size and speeds up inference by setting \"unimportant\" weights to 0.**. In this project, we proceeded implementing **Unstructured global Pruning - setting a portion** (40% by default) **of trained model weights with the smallest magnitudes (in absolute values) to 0**. The function below applies **globally (on ALL LINEAR / ALL LAYERS**, depending on user need), **on a fine-tuned HF model** (which has gone through the final training phases above), i.e. it looks for the portion of weights with the smallest magnitudes (in absolute values) and prunes them - sets them to 0. It then evaluates the pruned model on the test set, and saves the quantized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ek37jtu2_PYH",
   "metadata": {
    "id": "ek37jtu2_PYH"
   },
   "outputs": [],
   "source": [
    "# Critical roots\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "model_root   = f\"{project_root}/Model_Weights\"\n",
    "\n",
    "# Define pruned_root inside the project, for all pruned weights\n",
    "prune_root = f\"{project_root}/Pruned_Model_Weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G83_eLUmAB9W",
   "metadata": {
    "id": "G83_eLUmAB9W"
   },
   "outputs": [],
   "source": [
    "# This function prunes a fine-tuned HF model, evaluates it on training & test sets, compares its performance with the previous model's, and saves the pruned version.\n",
    "# is_linear = a boolean variable set by the user whether global unstructured pruning of is desired only across the linear layers, or across all model weights. False by default.\n",
    "def prune_evaluate_and_compare(model_name, model_name_dir, best_params, is_linear = False):\n",
    "\n",
    "  # Define original model path (trained weights) and pruned save path\n",
    "    model_path     = f\"{model_root}/{model_name_dir}\"\n",
    "    pruned_path = f\"{prune_root}/{model_name_dir}_pruned\"\n",
    "\n",
    "    # Select correct pretokenized dataset\n",
    "    if \"roberta\" in model_name_dir.lower():\n",
    "        pretokenized_dir = \"data/tokenized_twitter_roberta_base\" # the folder for saving the model\n",
    "    else:\n",
    "        pretokenized_dir = \"data/tokenized_bertweet_base\" # the folder for saving the model\n",
    "\n",
    "    # Load model & tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir) # Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # Merge train + validation for final training\n",
    "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
    "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
    "\n",
    "    # initialize loaders (train & test) from the pretokenized HF dataset\n",
    "    train_loader = DataLoader(\n",
    "        full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=2\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        ds[\"test\"], batch_size=min(2*best_params[\"batch_size\"], 128), shuffle=False,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate the performance of original model - first:\n",
    "    train_original = evaluate_model(model, train_loader, device)\n",
    "    test_original = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # Define the pruned model by reloading the original model\n",
    "    pruned_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "\n",
    "    # Collect layers to be pruned:\n",
    "    if is_linear:\n",
    "      parameters_to_prune = [(m, \"weight\") for m in pruned_model.modules() if isinstance(m, nn.Linear)] # if is_linear == True -> Unstructured-global-pruning only across linear layers\n",
    "    else:\n",
    "      parameters_to_prune = [(m, \"weight\") for m in pruned_model.modules() if hasattr(m, \"weight\")] # otherwise -> Unstructured-global-pruning only across linear layers\n",
    "\n",
    "    # Apply pruning (global, unstructured)\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=0.4 # set by default. Essentially, 40% of parameters of model's (linear / all, user-dependent as noted above) layers with the smallest magnitudes (absolute values) would be pruned - set to 0.\n",
    "    )\n",
    "\n",
    "    for m, n in parameters_to_prune:\n",
    "      prune.remove(m, n) # removing pruned weights (already set to 0) from the pruned model, to observe the actual parameter reduction.\n",
    "\n",
    "    # Save pruned model's config & weights\n",
    "    os.makedirs(pruned_path, exist_ok=True) # Create directory if it doesn't exist\n",
    "    pruned_model.save_pretrained(pruned_path)\n",
    "    # torch.save(pruned_model.state_dict(), os.path.join(pruned_path, \"pytorch_model.bin\"))\n",
    "    tokenizer.save_pretrained(pruned_path)\n",
    "\n",
    "    # Evaluate the performance of pruned model - second:\n",
    "    train_pruned = evaluate_model(pruned_model, train_loader, device)\n",
    "    test_pruned = evaluate_model(pruned_model, test_loader, device)\n",
    "\n",
    "    # Count number of parameters in both models - original & pruned:\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    pruned_params = sum(torch.count_nonzero(p).item() for p in pruned_model.parameters())\n",
    "\n",
    "    # Collect results into a DataFrame\n",
    "    results = pd.DataFrame([{\n",
    "        \"original_params\": original_params,\n",
    "        \"pruned_params\": pruned_params,\n",
    "        \"param_reduction\": original_params - pruned_params,\n",
    "        \"param_ratio\": pruned_params / original_params,\n",
    "        # Accuracy\n",
    "        \"train_accuracy_original\": train_original[\"accuracy\"],\n",
    "        \"test_accuracy_original\": test_original[\"accuracy\"],\n",
    "        \"train_accuracy_pruned\": train_pruned[\"accuracy\"],\n",
    "        \"test_accuracy_pruned\": test_pruned[\"accuracy\"],\n",
    "        \"train_accuracy_drop\": train_original[\"accuracy\"] - train_pruned[\"accuracy\"],\n",
    "        \"test_accuracy_drop\": test_original[\"accuracy\"] - test_pruned[\"accuracy\"],\n",
    "\n",
    "        # F1-Score\n",
    "        \"train_f1_original\": train_original[\"f1\"],\n",
    "        \"test_f1_original\": test_original[\"f1\"],\n",
    "        \"train_f1_pruned\": train_pruned[\"f1\"],\n",
    "        \"test_f1_pruned\": test_pruned[\"f1\"],\n",
    "        \"train_f1_drop\": train_original[\"f1\"] - train_pruned[\"f1\"],\n",
    "        \"test_f1_drop\": test_original[\"f1\"] - test_pruned[\"f1\"],\n",
    "\n",
    "        # Precision\n",
    "        \"train_precision_original\": train_original[\"precision\"],\n",
    "        \"test_precision_original\": test_original[\"precision\"],\n",
    "        \"train_precision_pruned\": train_pruned[\"precision\"],\n",
    "        \"test_precision_pruned\": test_pruned[\"precision\"],\n",
    "        \"train_precision_drop\": train_original[\"precision\"] - train_pruned[\"precision\"],\n",
    "        \"test_precision_drop\": test_original[\"precision\"] - test_pruned[\"precision\"],\n",
    "\n",
    "        # Recall\n",
    "        \"train_recall_original\": train_original[\"recall\"],\n",
    "        \"test_recall_original\": test_original[\"recall\"],\n",
    "        \"train_recall_pruned\": train_pruned[\"recall\"],\n",
    "        \"test_recall_pruned\": test_pruned[\"recall\"],\n",
    "        \"train_recall_drop\": train_original[\"recall\"] - train_pruned[\"recall\"],\n",
    "        \"test_recall_drop\": test_original[\"recall\"] - test_pruned[\"recall\"],\n",
    "    }], index=[model_name])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rFyZBYaV_Og3",
   "metadata": {
    "id": "rFyZBYaV_Og3"
   },
   "outputs": [],
   "source": [
    "# Pruning all 4 models with their corresponding batch sizes (typed manually!), considering ONLY LINEAR layers\n",
    "model_configs = {\n",
    "    \"BERTweet-Base (rec4)\": (\"best_model_bertweet_base_rec4\", 128),\n",
    "    \"BERTweet-Base (rec5 - HF)\": (\"best_model_bertweet_base_rec5\", 64),\n",
    "    \"RoBERTa-Base-Tweet (rec4)\": (\"best_model_roberta_base_tweet_rec4\", 128),\n",
    "    \"RoBERTa-Base-Tweet (rec5 - HF)\": (\"best_model_roberta_base_tweet_rec5\", 128)\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name, (model_name_dir, batch_size) in model_configs.items():\n",
    "    print(f\"\\nUnstructured global Pruning Results for {model_name} (considering linear layers only):\")\n",
    "    results_df = prune_evaluate_and_compare(model_name, model_name_dir, {\"batch_size\": batch_size}, is_linear = True)\n",
    "    results_df.index.name = \"model_name\"\n",
    "    all_results.append(results_df)\n",
    "    display(results_df)\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "all_results_df = pd.concat(all_results, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hJONIVOKlZ5B",
   "metadata": {
    "id": "hJONIVOKlZ5B"
   },
   "outputs": [],
   "source": [
    "# Display pruning results over all 4 models - \"LINEAR CASE\"\n",
    "display(all_results_df)\n",
    "\n",
    "# Save for future use\n",
    "save_path = f\"{prune_root}/pruning_results_linear.csv\"\n",
    "all_results_df.to_csv(save_path, index=True)\n",
    "print(f\"\\nAll unstructured global pruning results (considering linear layers only) saved to: {save_path} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZkyQJTEjkWWC",
   "metadata": {
    "id": "ZkyQJTEjkWWC"
   },
   "outputs": [],
   "source": [
    "# Pruning all 4 models with their corresponding batch sizes (typed manually!), considering ALL model layers\n",
    "all_results = []\n",
    "\n",
    "for model_name, (model_name_dir, batch_size) in model_configs.items():\n",
    "    print(f\"\\nUnstructured global Pruning Results for {model_name} (considering all model layers):\")\n",
    "    results_df = prune_evaluate_and_compare(model_name, model_name_dir, {\"batch_size\": batch_size}, is_linear = False)\n",
    "    results_df.index.name = \"model_name\"\n",
    "    all_results.append(results_df)\n",
    "    display(results_df)\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "all_results_df = pd.concat(all_results, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_wGv-j6u_Zu7",
   "metadata": {
    "id": "_wGv-j6u_Zu7"
   },
   "outputs": [],
   "source": [
    "# Display pruning results over all 4 models - \"GENERALIZED CASE\"\n",
    "display(all_results_df)\n",
    "\n",
    "# Save for future use\n",
    "save_path = f\"{prune_root}/pruning_results_generalized.csv\"\n",
    "all_results_df.to_csv(save_path, index=True)\n",
    "print(f\"\\nAll unstructured global pruning results (considering all model layers) saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5KJ5vpenIwKR",
   "metadata": {
    "id": "5KJ5vpenIwKR"
   },
   "source": [
    "## **Technique (3) - Knowledge-Distillation (KD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MM2928PKIw28",
   "metadata": {
    "id": "MM2928PKIw28"
   },
   "source": [
    "As a compression technique, **Knowledge Distillation** reduces model size by **training a compact student model** (compact = with much less parameters) **to imitate a stronger teacher model by matching the teacher’s soft predictions while still learning from the gold labels**. In our pipeline the fine-tuned teacher from Model_Weights is frozen and evaluated in eval() mode, and the student (e.g., arampacha/roberta-tiny) is optimized with the mixed objective α·CE(y, s) + (1−α)·T²·KL(softmax(t/T) || log_softmax(s/T)), where T is the temperature and α controls the balance between label supervision and teacher guidance. We train on the pre-tokenized datasets, log train and test metrics for both teacher and student to Weights & Biases, and save the best student checkpoint under KD_Model_Weights, along with a CSV of results for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DK3kk8cxIMrs",
   "metadata": {
    "id": "DK3kk8cxIMrs"
   },
   "outputs": [],
   "source": [
    "# Critical roots\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "model_root   = f\"{project_root}/Model_Weights\"\n",
    "\n",
    "# Define KD_root inside the project, for all KD weights\n",
    "KD_root = f\"{project_root}/KD_Model_Weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjeeIVRjjNqu",
   "metadata": {
    "id": "xjeeIVRjjNqu"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class TrainEvalCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, train_dataset):\n",
    "        self.trainer = trainer\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # HF already computed eval_* on the test set this epoch.\n",
    "        if metrics and wandb.run is not None:\n",
    "            out = {}\n",
    "            for k, v in metrics.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    key = k[5:] if k.startswith(\"eval_\") else k\n",
    "                    out[f\"test/student_{key}\"] = float(v)\n",
    "            wandb.log(out)\n",
    "        return control\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Compute train metrics once per epoch\n",
    "        train_metrics = self.trainer.evaluate(\n",
    "            eval_dataset=self.train_dataset,\n",
    "            metric_key_prefix=\"train\"  # -> train_loss, train_accuracy, ...\n",
    "        )\n",
    "\n",
    "        # 1) HF log keeps the pretty table\n",
    "        self.trainer.log(train_metrics)\n",
    "\n",
    "        # 1a) ALSO put 'loss' so the \"Training Loss\" column isn't \"No log\"\n",
    "        if \"train_loss\" in train_metrics:\n",
    "            self.trainer.log({\"loss\": float(train_metrics[\"train_loss\"])})\n",
    "\n",
    "        # 2) W&B: log under train/student_* (no explicit step)\n",
    "        if wandb.run is not None:\n",
    "            out = {}\n",
    "            for k, v in train_metrics.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    key = k[6:] if k.startswith(\"train_\") else k\n",
    "                    out[f\"train/student_{key}\"] = float(v)\n",
    "            wandb.log(out)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bJYMIUb1z-bB",
   "metadata": {
    "id": "bJYMIUb1z-bB"
   },
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # freeze + eval teacher\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.temperature = float(temperature)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _teacher_forward(self, **inputs):\n",
    "        # teacher never sees labels\n",
    "        inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
    "        return self.teacher(**inputs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1) remove labels so student model doesn't compute internal CE\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        if labels.dtype != torch.long:\n",
    "            labels = labels.long()\n",
    "        # cheap guard; safe even if always 0..4\n",
    "        if torch.any(labels.lt(0)) or torch.any(labels.gt(4)):\n",
    "            labels = labels.clamp_(0, 4)\n",
    "\n",
    "        # 2) student forward (no labels)\n",
    "        outputs_student = model(**inputs)\n",
    "\n",
    "        # 3) teacher forward (eval, no grads)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self._teacher_forward(**inputs)\n",
    "\n",
    "        # 4) KD loss\n",
    "        t = self.temperature\n",
    "        loss_ce = F.cross_entropy(outputs_student.logits, labels)\n",
    "        loss_kl = F.kl_div(\n",
    "            F.log_softmax(outputs_student.logits / t, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / t, dim=-1),\n",
    "            reduction=\"batchmean\"\n",
    "        ) * (t * t)\n",
    "\n",
    "        loss = self.alpha * loss_ce + (1.0 - self.alpha) * loss_kl\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iW3jIsX_wPvU",
   "metadata": {
    "id": "iW3jIsX_wPvU"
   },
   "outputs": [],
   "source": [
    "def distill_evaluate_and_compare(model_name, model_name_dir, best_params, student_model_name, alpha=0.5, temperature=2.0, num_epochs=5):\n",
    "\n",
    "    # Paths\n",
    "    teacher_path = f\"{model_root}/{model_name_dir}\"\n",
    "    student_slug = student_model_name.replace(\"/\", \"-\")\n",
    "    KD_path      = f\"{KD_root}/{model_name_dir}_distilled_student_{student_slug}\"\n",
    "    os.makedirs(KD_path, exist_ok=True)\n",
    "\n",
    "    # Select correct pretokenized dataset\n",
    "    if \"roberta\" in model_name_dir.lower():\n",
    "        pretokenized_dir = \"data/tokenized_twitter_roberta_base\"\n",
    "    else:\n",
    "        pretokenized_dir = \"data/tokenized_bertweet_base\"\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir)\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Merge train + validation for final training\n",
    "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]]).shuffle(seed=42)\n",
    "    test_dataset = ds[\"test\"]\n",
    "\n",
    "    # Load Teacher + tokenizer\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(teacher_path).to(device)\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(teacher_path)\n",
    "\n",
    "    # Load Student + tokenizer (vocab aligned)\n",
    "    student = AutoModelForSequenceClassification.from_pretrained(student_model_name, num_labels=5, ignore_mismatched_sizes=True).to(device)\n",
    "    student_tokenizer = tokenizer\n",
    "    student.resize_token_embeddings(len(student_tokenizer))\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # W&B init (project name includes teacher dir + student)\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n",
    "    wandb.init(\n",
    "        project=f\"KD_{model_name}__student_{student_slug}_21.8.2025\",\n",
    "        entity=\"idoshahar96-tel-aviv-university\",\n",
    "        config={\n",
    "            \"learning_rate\": best_params[\"learning_rate\"],\n",
    "            \"weight_decay\": best_params[\"weight_decay\"],\n",
    "            \"batch_size\": best_params[\"batch_size\"],\n",
    "            \"num_layers_finetune\": best_params.get(\"num_layers_finetune\", None),\n",
    "            \"teacher_path\": teacher_path,\n",
    "            \"student_model\": student_model_name,\n",
    "            \"alpha\": alpha,\n",
    "            \"temperature\": temperature,\n",
    "            \"epochs\": num_epochs,\n",
    "        },\n",
    "        name=f\"KD_{model_name}__{student_slug}\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    # TrainingArguments for the Hugging Face Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=KD_path, # where checkpoints will be saved\n",
    "        per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=best_params[\"batch_size\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        num_train_epochs=num_epochs, # Setting the number of epochs for training - 5\n",
    "        eval_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
    "        save_strategy=\"epoch\",        # save a checkpoint at the end of each epoch\n",
    "        logging_strategy=\"epoch\",     # log metrics at the end of each epoch\n",
    "        load_best_model_at_end=True,  # reload the best checkpoint (based on metric_for_best_model)\n",
    "        metric_for_best_model=\"accuracy\", # optimize w.r.t accuracy\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer_distill = DistillationTrainer(\n",
    "        model=student,\n",
    "        teacher_model=teacher,\n",
    "        args=training_args,\n",
    "        train_dataset=full_train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        temperature=temperature,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "\n",
    "    # per-epoch: TEST (from on_evaluate) then TRAIN (from on_epoch_end)\n",
    "    trainer_distill.add_callback(TrainEvalCallback(trainer_distill, full_train_dataset))\n",
    "\n",
    "    # TRAIN - KD\n",
    "    trainer_distill.train()\n",
    "    print(\"\\nDistillation complete. Student trained & best model loaded.\")\n",
    "\n",
    "    # Final metrics (TEST first, then TRAIN) for teacher & student\n",
    "    def _eval_with(model_to_eval, dataset, prefix):\n",
    "        tmp = Trainer(\n",
    "            model=model_to_eval,\n",
    "            args=training_args,\n",
    "            eval_dataset=dataset,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        return tmp.evaluate(metric_key_prefix=prefix)\n",
    "\n",
    "    # Teacher metrics\n",
    "    teacher_test   = _eval_with(teacher, test_dataset,        \"teacher_test\")\n",
    "    teacher_train  = _eval_with(teacher, full_train_dataset, \"teacher_train\")\n",
    "\n",
    "    # Student metrics\n",
    "    student_test   = trainer_distill.evaluate(eval_dataset=test_dataset,        metric_key_prefix=\"student_test\")\n",
    "    student_train  = trainer_distill.evaluate(eval_dataset=full_train_dataset, metric_key_prefix=\"student_train\")\n",
    "\n",
    "    # Log to W&B (TEST first, then TRAIN) so panels order naturally\n",
    "    if wandb.run is not None:\n",
    "        def log_group(prefix, who, d):\n",
    "            out = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    key = k.split(f\"{who}_\", 1)[-1] if f\"{who}_\" in k else k\n",
    "                    out[f\"{prefix}/{who}_{key}\"] = float(v)\n",
    "            wandb.log(out)\n",
    "\n",
    "        log_group(\"test\",  \"teacher\", teacher_test)\n",
    "        log_group(\"test\",  \"student\", student_test)\n",
    "        log_group(\"train\", \"teacher\", teacher_train)\n",
    "        log_group(\"train\", \"student\", student_train)\n",
    "\n",
    "    # Count params & save student model (best-epoch)\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    print(f\"Teacher params: {teacher_params:,}\")\n",
    "    print(f\"Student params: {student_params:,}\")\n",
    "    trainer_distill.model.save_pretrained(KD_path)\n",
    "    student_tokenizer.save_pretrained(KD_path)\n",
    "    print(f\"Best student model saved to {KD_path}\")\n",
    "    wandb.finish()\n",
    "\n",
    "    # Build output DataFrame (params statistics, then metrics, then drops in metrics while comparing the models)\n",
    "    def g(d, k):\n",
    "        v = d.get(k, float(\"nan\"))\n",
    "        try:\n",
    "            return float(v)\n",
    "        except Exception:\n",
    "            return float(\"nan\")\n",
    "\n",
    "    def pack(metric_name, t_train, t_test, s_train, s_test):\n",
    "        tt = g(t_train, f\"teacher_train_{metric_name}\")\n",
    "        te = g(t_test,  f\"teacher_test_{metric_name}\")\n",
    "        st = g(s_train, f\"student_train_{metric_name}\")\n",
    "        se = g(s_test,  f\"student_test_{metric_name}\")\n",
    "        return {\n",
    "            f\"teacher_train_{metric_name}\": tt,\n",
    "            f\"teacher_test_{metric_name}\":  te,\n",
    "            f\"student_train_{metric_name}\": st,\n",
    "            f\"student_test_{metric_name}\":  se,\n",
    "            f\"drop_train_{metric_name}\":    tt - st,  # teacher − student\n",
    "            f\"drop_test_{metric_name}\":     te - se,\n",
    "        }\n",
    "\n",
    "    row = {\n",
    "        \"teacher_params\": teacher_params,\n",
    "        \"student_params\": student_params,\n",
    "        \"param_reduction\": teacher_params - student_params,\n",
    "        \"param_ratio\": student_params / teacher_params,\n",
    "    }\n",
    "    for m in [\"accuracy\", \"f1\", \"precision\", \"recall\"]:\n",
    "        row.update(pack(m, teacher_train, teacher_test, student_train, student_test))\n",
    "\n",
    "    results = pd.DataFrame([row], index=[model_name])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R2bpF5Z5JDu9",
   "metadata": {
    "id": "R2bpF5Z5JDu9"
   },
   "outputs": [],
   "source": [
    "# Performing Knowledge-Distillation (KD) over all 4 models with their corresponding best-params (typed manually!)\n",
    "model_configs = {\n",
    "    \"BERTweet-Base (rec4)\": (\"best_model_bertweet_base_rec4\", {'learning_rate': 0.0001184412471705182, 'weight_decay': 1.2699696348040995e-05, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3}),\n",
    "    \"BERTweet-Base (rec5 - HF)\": (\"best_model_bertweet_base_rec5\", {'learning_rate': 7.668855564109297e-05, 'weight_decay': 4.8978169582912055e-06, 'patience': 9, 'batch_size': 64, 'num_layers_finetune': 3, 'lr_scheduler_type': 'linear'}),\n",
    "    \"RoBERTa-Base-Tweet (rec4)\": (\"best_model_roberta_base_tweet_rec4\", {'learning_rate': 0.0003834791389042033, 'weight_decay': 2.88286253103848e-06, 'patience': 7, 'batch_size': 128, 'num_layers_finetune': 3}),\n",
    "    \"RoBERTa-Base-Tweet (rec5 - HF)\": (\"best_model_roberta_base_tweet_rec5\", {'learning_rate': 0.0000860370374400373, 'weight_decay': 0.00008459884214639005, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3, 'lr_scheduler_type': 'polynomial'})\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "student_model_name = \"arampacha/roberta-tiny\" # Example student model - RoBERTa-Tiny (truly)\n",
    "\n",
    "for model_name, (model_name_dir, best_params) in model_configs.items():\n",
    "    print(f\"\\nKnowledge-Distillation (KD) Results for TEACHER: {model_name}, STUDENT: {student_model_name} (5 epochs):\")\n",
    "    results_df = distill_evaluate_and_compare(model_name, model_name_dir, best_params, student_model_name=student_model_name)\n",
    "    results_df.index.name = \"model_name\"\n",
    "    all_results.append(results_df)\n",
    "    display(results_df)\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "all_results_df = pd.concat(all_results, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QULDcH4GJUIK",
   "metadata": {
    "id": "QULDcH4GJUIK"
   },
   "outputs": [],
   "source": [
    "# Add student name column (redundant because the student model name would be clear from the CSV file directory, but we wanted to make the results even clearer, by displaying the student model name even more explicitly)\n",
    "all_results_df[\"student_model_name\"] = \"arampacha-roberta-tiny\"\n",
    "\n",
    "# Reorder so \"student_model_name\" is right after the index\n",
    "cols = all_results_df.columns.tolist()\n",
    "cols = [\"student_model_name\"] + [c for c in cols if c != \"student_model_name\"]\n",
    "all_results_df = all_results_df[cols]\n",
    "all_results_df[\"student_model_name\"] = \"arampacha-roberta-tiny\"\n",
    "# Display Knowledge-Distillation (KD) results over all 4 models\n",
    "display(all_results_df)\n",
    "\n",
    "student_model_name_for_csv = \"arampacha_roberta_tiny\"\n",
    "\n",
    "# Save for future use\n",
    "save_path = f\"{KD_root}/KD_results_{student_model_name_for_csv}.csv\"\n",
    "all_results_df.to_csv(save_path, index=True)\n",
    "print(f\"\\nAll Knowledge-Distillation (KD) results saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
