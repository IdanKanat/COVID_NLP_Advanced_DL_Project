{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713c48a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IdanKanat/COVID_NLP_Advanced_DL_Project/blob/main/AdvancedTopicsDL_Project_IdanKanat%26IdoShahar_COVID_NLP_21.8.2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0819caac-d47c-4d19-9ad6-6ed12fd15e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: optuna in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: wandb in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.34.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2025.7.14)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57412\\1376978527.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./01_EDA_and_data_preprocessing.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./02_finetune_WITHOUT_HF_Trainer.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:741\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    740\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 741\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39msafe_execfile_ipy(filename, raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3005\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   3003\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 3005\u001b[0m     result\u001b[38;5;241m.\u001b[39mraise_error()\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   3007\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57412\\1376978527.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "%run ./01_EDA_and_data_preprocessing.ipynb\n",
    "%run ./02_finetune_WITHOUT_HF_Trainer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dzBtXCucn2JO",
   "metadata": {
    "id": "dzBtXCucn2JO"
   },
   "source": [
    "## **HP Tuning using HuggingFace Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N75YY4k7JYao",
   "metadata": {
    "id": "N75YY4k7JYao"
   },
   "outputs": [],
   "source": [
    "# Load evaluation metrics, using the evaluate library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Compute metrics function for the Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gOXEEYcgnwLc",
   "metadata": {
    "id": "gOXEEYcgnwLc"
   },
   "outputs": [],
   "source": [
    "# Objective function for Optuna hyperparameter tuning\n",
    "def objective_HF(trial, architecture):\n",
    "\n",
    "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
    "    if architecture == \"twitter-roberta-base\":\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
    "    else:\n",
    "        model_name = \"vinai/bertweet-base\"\n",
    "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5) # 5 labels for the 5 sentiments\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    base_model = model.roberta # Base model for both models (RoBERTa-Base-Tweet & BERTweet-Base) - RoBERTa\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    patience = trial.suggest_int(\"patience\", 7, 10)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 3)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"polynomial\"])\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # Freezing and Unfreezing layers\n",
    "    for p in base_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if num_layers_finetune > 0:  # safety guard: avoid the \"-0\" edge case\n",
    "        for p in base_model.encoder.layer[-num_layers_finetune:].parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    if wandb.run is not None:\n",
    "      wandb.finish() # Check if W&B doesn't run anything in parallel. If so, stop the pre-existing run.\n",
    "\n",
    "   # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(\n",
    "        project=f\"{architecture}_HF_CORONA_NLP_Twitter_Sentiment_Analysis_14.8.2025_FULL_HP_TUNING\",\n",
    "        entity=\"idoshahar96-tel-aviv-university\",\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"patience\": patience,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_layers_finetune\": num_layers_finetune,\n",
    "            \"lr_scheduler_type\": lr_scheduler_type,\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"\n",
    "        },\n",
    "        name=f\"trial_{trial.number}\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    # TrainingArguments for the Hugging Face Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"HF-results/trial_{trial.number}\",  # where checkpoints will be saved\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=20,           # Setting the number of epochs for training - 20\n",
    "        eval_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
    "        save_strategy=\"epoch\",        # save a checkpoint at the end of each epoch\n",
    "        logging_strategy=\"epoch\",     # log metrics at the end of each epoch\n",
    "        load_best_model_at_end=True,  # reload the best checkpoint (based on metric_for_best_model)\n",
    "        metric_for_best_model=\"accuracy\", # optimize w.r.t accuracy\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,           # keep only the best checkpoint\n",
    "        report_to=\"wandb\",            # log to Weights & Biases\n",
    "        lr_scheduler_type=lr_scheduler_type\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train_reduced\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save best trial results\n",
    "    trainer.save_model(f\"HF-results/trial_{trial.number}\")  # ensures config.json + weights are there\n",
    "    tokenizer.save_pretrained(f\"HF-results/trial_{trial.number}\")\n",
    "\n",
    "    # Evaluate the best model on the validation set\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    wandb.finish()\n",
    "\n",
    "    # Optuna uses the validation accuracy as the optimization target\n",
    "    acc = eval_metrics.get(\"eval_accuracy\", 0.0)\n",
    "    if np.isnan(acc):\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uZ_rBd0wwiCO",
   "metadata": {
    "id": "uZ_rBd0wwiCO"
   },
   "source": [
    "#### **RoBERTa-Base-Tweet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qMuNLFT1Npp7",
   "metadata": {
    "id": "qMuNLFT1Npp7"
   },
   "outputs": [],
   "source": [
    "# Creating an Optuna Study - RoBERTa -Base-Tweet (rec5):\n",
    "study_roberta_base_tweet_rec5 = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
    "study_roberta_base_tweet_rec5.optimize(lambda trial: objective_HF(trial, \"twitter-roberta-base\"), n_trials=12) # Specified 12 trials\n",
    "\n",
    "print(\"Best objective value (validation accuracy):\", study_roberta_base_tweet_rec5.best_value)\n",
    "print(\"The chosen HP combination:\", study_roberta_base_tweet_rec5.best_params)\n",
    "print(\"Trial number of the best objective (validation accuracy) value:\", study_roberta_base_tweet_rec5.best_trial.number)\n",
    "\n",
    "# Define the path to save the file in Google Drive with REC5 naming\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "hp_root = f\"{project_root}/Model_HPs\"\n",
    "drive_path = f\"{hp_root}/best_model_roberta_base_tweet_rec5_hyperparams.json\"\n",
    "\n",
    "with open(drive_path, \"w\") as f:\n",
    "    json.dump(study_roberta_base_tweet_rec5.best_params, f)\n",
    "\n",
    "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zZCQF523wtvu",
   "metadata": {
    "id": "zZCQF523wtvu"
   },
   "source": [
    "#### **BerTweet-Base:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fePovRGhNfpc",
   "metadata": {
    "id": "fePovRGhNfpc"
   },
   "outputs": [],
   "source": [
    "# Creating an Optuna Study - BerTweet-Base (rec5):\n",
    "study_bertweet_base_rec5 = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
    "study_bertweet_base_rec5.optimize(lambda trial: objective_HF(trial, \"bertweet-base\"), n_trials=12) # Specified 12 trials\n",
    "\n",
    "print(\"Best objective value (validation accuracy):\", study_bertweet_base_rec5.best_value)\n",
    "print(\"The chosen HP combination:\", study_bertweet_base_rec5.best_params)\n",
    "print(\"Trial number of the best objective (validation accuracy) value:\", study_bertweet_base_rec5.best_trial.number)\n",
    "\n",
    "# Define the path to save the file in Google Drive with REC5 naming\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "hp_root = f\"{project_root}/Model_HPs\"\n",
    "drive_path = f\"{hp_root}/best_model_bertweet_base_rec5_hyperparams.json\"\n",
    "\n",
    "with open(drive_path, \"w\") as f:\n",
    "    json.dump(study_bertweet_base_rec5.best_params, f)\n",
    "\n",
    "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAD3Ed_AelJs",
   "metadata": {
    "id": "LAD3Ed_AelJs"
   },
   "source": [
    "## **Final Training using HuggingFace Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FKPi7ArMghQx",
   "metadata": {
    "id": "FKPi7ArMghQx"
   },
   "source": [
    "After finding the best trial (hyperparameter combination) using the objective-HF function, the `train_model_with_hyperparams_HF` is called for final model training using the obtained hyperparameter combination. It appears similar to the way we trained each model under each trial specification in the Optuna based objective-HF function. This additional function supports model saving too, and generalized for each model architecture. It's worth noting that in practice, the validation dataset in this function would be the actual test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vBgPa9QLTUUx",
   "metadata": {
    "id": "vBgPa9QLTUUx"
   },
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams_HF(architecture, best_params, save_path):\n",
    "\n",
    "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
    "    if architecture == \"twitter-roberta-base\":\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
    "    else:\n",
    "        model_name = \"vinai/bertweet-base\"\n",
    "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5) # 5 labels for the 5 sentiments\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    base_model = model.roberta # Base model for both models (RoBERTa-Base-Tweet & BERTweet-Base) - RoBERTa\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Merge train + validation for final training\n",
    "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
    "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # Freezing and Unfreezing layers\n",
    "    for p in base_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if best_params[\"num_layers_finetune\"] > 0:  # safety guard: avoid the \"-0\" edge case\n",
    "        for p in base_model.encoder.layer[-best_params[\"num_layers_finetune\"]:].parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    if wandb.run is not None:\n",
    "      wandb.finish() # Check if W&B doesn't run anything in parallel. If so, stop the pre-existing run.\n",
    "\n",
    "   # Initialize Weights & Biases - the values in the config are the properties of the best trial found in the Optuna-HP-Tuning step.\n",
    "    wandb.init(\n",
    "        project=f\"{architecture}_HF_CORONA_NLP_Twitter_Sentiment_Analysis_19.8.2025_FULL_TRAINING\",\n",
    "        entity=\"idoshahar96-tel-aviv-university\",\n",
    "        config={\n",
    "            \"learning_rate\": best_params[\"learning_rate\"],\n",
    "            \"weight_decay\": best_params[\"weight_decay\"],\n",
    "            \"patience\": best_params[\"patience\"],\n",
    "            \"batch_size\": best_params[\"batch_size\"],\n",
    "            \"num_layers_finetune\": best_params[\"num_layers_finetune\"],\n",
    "            \"lr_scheduler_type\": best_params[\"lr_scheduler_type\"],\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"\n",
    "        },\n",
    "        name=\"FINAL_TRAINING\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    # TrainingArguments for the Hugging Face Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_path,  # where checkpoints will be saved\n",
    "        per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "        per_device_eval_batch_size=best_params[\"batch_size\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        num_train_epochs=25,           # Setting the number of epochs for training - 25\n",
    "        eval_strategy=\"epoch\",        # evaluate at the end of each epoch\n",
    "        save_strategy=\"epoch\",        # save a checkpoint at the end of each epoch\n",
    "        logging_strategy=\"epoch\",     # log metrics at the end of each epoch\n",
    "        load_best_model_at_end=True,  # reload the best checkpoint (based on metric_for_best_model)\n",
    "        metric_for_best_model=\"accuracy\", # optimize w.r.t accuracy\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,           # keep only the best checkpoint\n",
    "        report_to=\"wandb\",            # log to Weights & Biases\n",
    "        lr_scheduler_type=best_params[\"lr_scheduler_type\"]\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=full_train_dataset,\n",
    "        eval_dataset=ds[\"test\"], # Evaluating the model using the test dataset\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=best_params[\"patience\"])]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    trainer.save_model(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yqao3LoXiH_O",
   "metadata": {
    "id": "Yqao3LoXiH_O"
   },
   "source": [
    "#### **RoBERTa-Base-Tweet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TuvWwi3tio7F",
   "metadata": {
    "id": "TuvWwi3tio7F"
   },
   "outputs": [],
   "source": [
    "# best_params = study_roberta_base_tweet_rec5.best_params  # get best HPs from the model's Optuna study\n",
    "best_params = {'learning_rate': 0.0000860370374400373, 'weight_decay': 0.00008459884214639005, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3, 'lr_scheduler_type': 'polynomial'} # Manually typed the best_params for future use\n",
    "name_path = \"/best_model_roberta_base_tweet_rec5\"\n",
    "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
    "\n",
    "# Training the Model (1), using Optuna-study's best trial HPs - RoBERTa-Base-Tweet:\n",
    "train_model_with_hyperparams_HF(architecture=\"twitter-roberta-base\", best_params=best_params,save_path=save_path)\n",
    "\n",
    "# Zip the whole model folder\n",
    "shutil.make_archive(save_path, \"zip\", save_path)\n",
    "\n",
    "# Download the zip to your computer\n",
    "files.download(f\"{save_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OpdCqyAHiMf4",
   "metadata": {
    "id": "OpdCqyAHiMf4"
   },
   "source": [
    "#### **BerTweet-Base:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MaWtswBPTW6h",
   "metadata": {
    "id": "MaWtswBPTW6h"
   },
   "outputs": [],
   "source": [
    "# best_params = study_bertweet_base_rec5.best_params  # get best HPs from the model's Optuna study\n",
    "best_params = {'learning_rate': 7.668855564109297e-05, 'weight_decay': 4.8978169582912055e-06, 'patience': 9, 'batch_size': 64, 'num_layers_finetune': 3, 'lr_scheduler_type': 'linear'} # Manually typed the best_params for future use\n",
    "name_path = \"/best_model_bertweet_base_rec5\"\n",
    "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
    "\n",
    "# Training the Model (2), using Optuna-study's best trial HPs - BERTweet-Base:\n",
    "train_model_with_hyperparams_HF(architecture=\"bertweet-base\", best_params=best_params,save_path=save_path)\n",
    "\n",
    "# Zip the whole model folder\n",
    "shutil.make_archive(save_path, \"zip\", save_path)\n",
    "\n",
    "# Download the zip to your computer\n",
    "files.download(f\"{save_path}.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
