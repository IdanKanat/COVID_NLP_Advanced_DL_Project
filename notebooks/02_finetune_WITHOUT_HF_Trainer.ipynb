{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713c48a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IdanKanat/COVID_NLP_Advanced_DL_Project/blob/main/AdvancedTopicsDL_Project_IdanKanat%26IdoShahar_COVID_NLP_21.8.2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84e0b3d-86ae-4591-bc0a-8914fcab133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: optuna in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: wandb in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.34.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2025.7.14)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\1376978527.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./01_EDA_and_data_preprocessing.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:741\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    740\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 741\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39msafe_execfile_ipy(filename, raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3005\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   3003\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 3005\u001b[0m     result\u001b[38;5;241m.\u001b[39mraise_error()\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   3007\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\1376978527.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "%run ./01_EDA_and_data_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3JdfNhoZFq",
   "metadata": {
    "id": "9f3JdfNhoZFq"
   },
   "source": [
    "## **Importing the Models - *RoBERTa-Base-Tweet* and *BERTweet-Base***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c6608",
   "metadata": {
    "id": "cc1c6608"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model from Hugging Face\n",
    "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer_twitter_roberta_base = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the first model from HuggingFace - ROBERTA Transformer Encoder, fine-tuned for sentiment analysis from tweets:\n",
    "roberta_tweets_1_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base\", num_labels = 5 # 5 labels for the 5 sentiments\n",
    ").to(device)\n",
    "roberta_tweets_1_model # glancing at the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40f296",
   "metadata": {
    "id": "5b40f296"
   },
   "source": [
    "## **Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd94e6",
   "metadata": {
    "id": "96cd94e6"
   },
   "source": [
    "### **Tweet Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac68d5b",
   "metadata": {
    "id": "eac68d5b"
   },
   "outputs": [],
   "source": [
    "# Defining the TweetDataset class with 3 built in functions (init, len and getitem) for integration with the PyTorch DataLoader object\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing the data\n",
    "            tokenizer: HuggingFace tokenizer for text processing\n",
    "            max_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "\n",
    "        self.texts = dataframe['CleanTweet'].tolist()\n",
    "        self.labels = dataframe['Sentiment'].map(label2id).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"text\": self.texts[idx], \"label\": self.labels[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c09025",
   "metadata": {
    "id": "01c09025"
   },
   "source": [
    "### **Early Stopping Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3037fec",
   "metadata": {
    "id": "e3037fec"
   },
   "outputs": [],
   "source": [
    "# Check for early stopping, applied for regularization. If the relevant validation metric (accuracy) shows no observable\n",
    "# improvement (w.r.t best observed val metric up until now) over several epochs consecutively, model training stops.\n",
    "# This function outputs the best_val_accuracy, epoch & early stop flag for each epoch it's called\n",
    "def early_stop_check(patience, best_val_accuracy, best_val_accuracy_epoch, current_val_accuracy, current_val_accuracy_epoch):\n",
    "    early_stop_flag = False\n",
    "    if current_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = current_val_accuracy\n",
    "        best_val_accuracy_epoch = current_val_accuracy_epoch\n",
    "    else:\n",
    "        if current_val_accuracy_epoch - best_val_accuracy_epoch > patience:\n",
    "            early_stop_flag = True\n",
    "    return best_val_accuracy, best_val_accuracy_epoch, early_stop_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072faeb0",
   "metadata": {
    "id": "072faeb0"
   },
   "source": [
    "## **Model Training**\n",
    "    The train_model_with_hyperparams function trains the model using the given training and validation loaders,\n",
    "    with early stopping.\n",
    "    Logs training and validation performance to Weights & Biases (accuracy, precision, recall, F1-score, and confusion matrix).\n",
    "    Returns the best model validation loss and saves the best model checkpoint per trial.\n",
    "\n",
    "      Args:\n",
    "        model (.from_pretrained): Transformer encoder model, imported from HuggingFace\n",
    "        train_loader (DataLoader): DataLoader for training data\n",
    "        val_loader (DataLoader): DataLoader for validation data\n",
    "        optimizer (torch.optim.Optimizer): Optimizer\n",
    "        criterion (nn.Module): Loss function\n",
    "        epochs (int): Max number of epochs\n",
    "        patience (int): Early stopping patience\n",
    "        trial (optuna.trial.Trial): Current Optuna trial\n",
    "    Returns:\n",
    "        float: Best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e18eb0",
   "metadata": {
    "id": "e2e18eb0"
   },
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
    "    # speed toggles (safe to call each time)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_val_accuracy = 0.0 # Initialize best validation accuracy\n",
    "    best_val_accuracy_epoch = 0 # Track epoch with the best validation accuracy\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None # To save the best model (in each trial / final training)\n",
    "\n",
    "    device_ = next(model.parameters()).device  # robust device grab\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Enable training mode\n",
    "        train_loss = 0.0 # Initializing the cumulative training loss for the current epoch to 0.\n",
    "        total_train = 0 # Initialize total_train here\n",
    "        correct_train = 0 # Initialize correct_train here\n",
    "\n",
    "        train_preds = [] # Store predicted classes for metrics\n",
    "        train_targets = []  # Store true labels for metrics\n",
    "\n",
    "        for batch in train_loader: # Iterates over the train_loader, which is a DataLoader object containing batches of training data. Each iteration yields a batch of inputs (images) and corresponding labels (ground-truth classes).\n",
    "            # Non-blocking H2D copies (works best with pin_memory=True on DataLoader)\n",
    "            input_ids      = batch[\"input_ids\"].to(device_, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device_, non_blocking=True)\n",
    "            labels         = batch[\"labels\"].to(device_, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "\n",
    "\n",
    "            # AMP forward/backward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits  = outputs.logits\n",
    "                loss    = criterion(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # (Optional) gradient clipping for extra stability:\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "            # Compute metrics\n",
    "            bs = labels.size(0)\n",
    "            train_loss += loss.item() * bs\n",
    "            total_train += bs\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            train_preds.extend(preds.detach().cpu().numpy())\n",
    "            train_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        train_loss /= max(total_train, 1)\n",
    "        train_accuracy = correct_train / max(total_train, 1)\n",
    "        train_f1 = f1_score(train_targets, train_preds, average='macro', zero_division=0)\n",
    "        train_precision = precision_score(train_targets, train_preds, average='macro', zero_division=0)\n",
    "        train_recall = recall_score(train_targets, train_preds, average='macro', zero_division=0)\n",
    "\n",
    "        # Validation check\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss_sum = 0.0\n",
    "            total_val = 0\n",
    "            correct_val = 0\n",
    "            val_preds, val_targets = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids      = batch[\"input_ids\"].to(device_, non_blocking=True)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device_, non_blocking=True)\n",
    "                    labels         = batch[\"labels\"].to(device_, non_blocking=True)\n",
    "\n",
    "                    # AMP also speeds up eval\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                        logits  = outputs.logits\n",
    "                        loss    = criterion(logits, labels)\n",
    "\n",
    "                    bs = labels.size(0)\n",
    "                    val_loss_sum += loss.item() * bs\n",
    "                    total_val += bs\n",
    "\n",
    "                    preds = logits.argmax(dim=1)\n",
    "                    correct_val += (preds == labels).sum().item()\n",
    "                    val_preds.extend(preds.detach().cpu().numpy())\n",
    "                    val_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "            val_loss = val_loss_sum / max(total_val, 1)\n",
    "            val_accuracy = correct_val / max(total_val, 1)\n",
    "            val_precision = precision_score(val_targets, val_preds, average='macro', zero_division=0)\n",
    "            val_recall = recall_score(val_targets, val_preds, average='macro', zero_division=0)\n",
    "            val_f1 = f1_score(val_targets, val_preds, average='macro', zero_division=0)\n",
    "\n",
    "            # Check for Early stopping (& updates best_val_accuracy & epoch)\n",
    "            if patience is not None:\n",
    "                best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
    "                    patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch\n",
    "                )\n",
    "\n",
    "            # Save best-so-far weights (>= to handle ties)\n",
    "            if val_accuracy >= best_val_accuracy and total_val > 0:\n",
    "                best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "            # W & B logging (if active)\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Train Loss\": train_loss,\n",
    "                    \"Train Accuracy\": train_accuracy,\n",
    "                    \"Train F1 Score\": train_f1,\n",
    "                    \"Train Precision\": train_precision,\n",
    "                    \"Train Recall\": train_recall,\n",
    "                    \"Validation Loss\": val_loss,\n",
    "                    \"Validation Accuracy\": val_accuracy,\n",
    "                    \"Validation Precision\": val_precision,\n",
    "                    \"Validation Recall\": val_recall,\n",
    "                    \"Validation F1\": val_f1,\n",
    "                })\n",
    "\n",
    "            if early_stop_flag:\n",
    "                break\n",
    "\n",
    "    # Save best model weights (if we ever improved)\n",
    "    # if best_model_state is not None:\n",
    "        # torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\")\n",
    "\n",
    "    # Restore best weights into the model before returning best_val_accuracy\n",
    "    if best_model_state is not None:\n",
    "      model.load_state_dict(best_model_state)\n",
    "\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7ceeb",
   "metadata": {
    "id": "e4e7ceeb"
   },
   "source": [
    "## **HP Tuning without HuggingFace's Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f0897-77a6-4117-ab44-c420f0142103",
   "metadata": {
    "id": "292f0897-77a6-4117-ab44-c420f0142103"
   },
   "source": [
    "Optuna objective function for tuning the given Transformer encoder model on twitter data.\n",
    "\n",
    "Each trial runs training with a different set of hyperparameters and logs key training & validation metrics to Weights & Biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacbbd9-0f22-45bc-8219-b4ff8b4280cf",
   "metadata": {
    "id": "7cacbbd9-0f22-45bc-8219-b4ff8b4280cf"
   },
   "outputs": [],
   "source": [
    "# Objective Function for Optuna:\n",
    "def objective(trial, architecture):\n",
    "\n",
    "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
    "    if architecture == \"twitter-roberta-base\":\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = model.roberta\n",
    "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
    "    else:\n",
    "        model_name = \"vinai/bertweet-base\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = model.roberta\n",
    "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
    "\n",
    "\n",
    "    # Hyperparameter suggestions:\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    patience = trial.suggest_int(\"patience\", 7, 10)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 3)\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # build loaders from the pretokenized HF dataset\n",
    "    train_loader = DataLoader(\n",
    "        ds[\"train_reduced\"], batch_size=batch_size, shuffle=True,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ds[\"validation\"], batch_size=min(2*batch_size, 128), shuffle=False,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    #Freezing and Unfreezing layers\n",
    "    for p in base_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if num_layers_finetune > 0:  # safety guard: avoid the \"-0\" edge case\n",
    "        for p in base_model.encoder.layer[-num_layers_finetune:].parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(project=f\"{architecture}_CORONA_NLP_Twitter_Sentiment_Analysis_13.8.2025_FULL_HP_TUNING\",\n",
    "               entity = \"idoshahar96-tel-aviv-university\",\n",
    "               config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"patience\": patience,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_layers_finetune\": num_layers_finetune,\n",
    "        \"architecture\": architecture,\n",
    "        \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"},\n",
    "        name=f\"trial_{trial.number}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best validation accuracy\n",
    "    best_val_accuracy = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=15, patience=patience, trial=trial)\n",
    "\n",
    "    wandb.finish() # Finish the Weights & Biases run\n",
    "\n",
    "    return best_val_accuracy # Return best validation accuracy as the objective to maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e457c-416b-4b5c-9a57-2d0bf269113f",
   "metadata": {
    "id": "2d7e457c-416b-4b5c-9a57-2d0bf269113f"
   },
   "source": [
    "## **Pre-tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9f9df-f196-4a9a-9096-c298dc9679ed",
   "metadata": {
    "id": "afc9f9df-f196-4a9a-9096-c298dc9679ed"
   },
   "source": [
    "Tokenization is CPU-heavy. If we do it all over again in each of the Optuna trials, then re-tokenizing wastes time.\n",
    "Pre-tokenization makes that cost zero for subsequent runs.\n",
    "\n",
    "It includes:\n",
    "running the tokenizer once over the whole dataset and applying truncation with a fixed ceiling MAX_LEN.\n",
    "For each sample i, we store a variable-length vector called len_i, which will be the min(original_len_i, MAX_LEN).\n",
    "Each saved sample can have a different length.\n",
    "You save the result to disk (Arrow format) with the columns of input_ids, attention_mask, and labels\n",
    "So, after this step, no trial needs to call the tokenizer and every trial just loads these IDs.\n",
    "\n",
    "In this step we are doing padding at all:\n",
    "The DataLoader pulls a batch from the disk. Then, the collator looks at the lengths in that batch, finds the longest sequence in each batch, and pads only up to this length.\n",
    "This is dynamic padding: it happens per batch, at runtime, and never re-tokenizes—it only adds pad tokens so tensors in the batch share the same shape\n",
    "The Dynamic padding keeps tensors tight to the batch’s real lengths → fewer pad tokens → fewer FLOPs in the model’s forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c80dd-9014-471d-87cf-d55954a5cbe8",
   "metadata": {
    "collapsed": true,
    "id": "136c80dd-9014-471d-87cf-d55954a5cbe8",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting the sentiment labels into integers via label2id, and drops the original Sentiment column\n",
    "train_df_reduced_ = train_df_reduced.assign(label=train_df_reduced[\"Sentiment\"].map(label2id)).drop(columns=[\"Sentiment\"])\n",
    "val_df_ = val_df.assign(label=val_df[\"Sentiment\"].map(label2id)).drop(columns=[\"Sentiment\"])\n",
    "test_df_ = test_df.assign(label=test_df[\"Sentiment\"].map(label2id)).drop(columns=[\"Sentiment\"])\n",
    "\n",
    "# SANITY CHECK - to make sure our training works, we added this code to make sure the training works on little training & validation data (as well as few trials & epochs per trial).\n",
    "# train_df_reduced_ = train_df_reduced_.sample(n=300, random_state=42)  # pick only 300 training rows\n",
    "# val_df_ = val_df_.sample(n=100, random_state=42)                      # pick only 100 validation rows\n",
    "# test_df_ = test_df_.sample(n=100, random_state=42)                    # optional: smaller test set too\n",
    "\n",
    "# Converting the Pandas DataFrames to HuggingFace Datasets and wraping them in a DatasetDict\n",
    "raw_ds = DatasetDict({\n",
    "    \"train_reduced\": Dataset.from_pandas(train_df_reduced_, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_df_, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df_, preserve_index=False),\n",
    "})\n",
    "\n",
    "\n",
    "def pretokenize_one(model_name: str, save_dir: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    # compute a single cap once (same idea you used inside objective)\n",
    "    enc_tmp  = tok(train_df_reduced[\"CleanTweet\"].tolist(), truncation=False)\n",
    "    lengths  = [len(x) for x in enc_tmp[\"input_ids\"]]\n",
    "    MAX_LEN  = max(64, min(int(np.percentile(lengths, 95)), 128))\n",
    "    print(f\"[{model_name}] MAX_LEN={MAX_LEN}\")\n",
    "\n",
    "    # tokenize (NO padding) and save\n",
    "    tokenized = raw_ds.map(\n",
    "        lambda b: tok(b[\"CleanTweet\"], truncation=True, max_length=MAX_LEN, padding=False),\n",
    "        batched=True, remove_columns=[\"CleanTweet\"]\n",
    "    )\n",
    "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "    tokenized.save_to_disk(save_dir)\n",
    "    print(f\"Saved to: {save_dir}\")\n",
    "\n",
    "# Run once per architecture you plan to use:\n",
    "pretokenize_one(\"cardiffnlp/twitter-roberta-base\", \"data/tokenized_twitter_roberta_base\")\n",
    "pretokenize_one(\"vinai/bertweet-base\",          \"data/tokenized_bertweet_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dbdca-6989-42bf-bbe6-9fc88f742475",
   "metadata": {
    "id": "c09dbdca-6989-42bf-bbe6-9fc88f742475"
   },
   "source": [
    "### **Running the Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bd629-4756-4db5-9658-a6d53f33afb8",
   "metadata": {
    "id": "7a5bd629-4756-4db5-9658-a6d53f33afb8"
   },
   "source": [
    "#### ***Model (1) - RoBERTa-Base-Tweet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af89ab",
   "metadata": {
    "id": "37af89ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating an Optuna Study - RoBERTa-Base-Tweet (rec4):\n",
    "study = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
    "study.optimize(lambda trial: objective(trial, \"twitter-roberta-base\"), n_trials=10) # Specified 10 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gy5B8ZvlYO07",
   "metadata": {
    "id": "gy5B8ZvlYO07"
   },
   "outputs": [],
   "source": [
    "# Documenting best hyperparameter combination - first model - RoBERTa-Base-Tweet - Rec4 code:\n",
    "study_roberta_base_tweet_rec4 = study\n",
    "print(\"Best objective value (validation accuracy):\", study.best_value)\n",
    "print(\"The chosen HP combination:\", study.best_params)\n",
    "print(\"Trial number of the best objective (validation accuracy) value:\", study.best_trial.number)\n",
    "\n",
    "print(\"Best objective value (validation accuracy):\", study_roberta_base_tweet_rec4.best_value)\n",
    "print(\"The chosen HP combination:\", study_roberta_base_tweet_rec4.best_params)\n",
    "print(\"Trial number of the best objective (validation accuracy) value:\", study_roberta_base_tweet_rec4.best_trial.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ELU2fW_c2WQR",
   "metadata": {
    "id": "ELU2fW_c2WQR"
   },
   "outputs": [],
   "source": [
    "# Define the path to save the file in Google Drive with REC4 naming\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "hp_root = f\"{project_root}/Model_HPs\"\n",
    "drive_path = f\"{hp_root}/best_roberta_base_tweet_rec4_hyperparams.json\"\n",
    "\n",
    "with open(drive_path, \"w\") as f:\n",
    "    # json.dump(study_bertweet_base_rec4.best_params, f)\n",
    "    json.dump({'learning_rate': 0.0003834791389042033, 'weight_decay': 2.88286253103848e-06, 'patience': 7, 'batch_size': 128, 'num_layers_finetune': 3}, f) # Manually typed the best_params for future use\n",
    "\n",
    "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e814d-6f75-4764-8a27-0b41ab022eb8",
   "metadata": {
    "id": "bc7e814d-6f75-4764-8a27-0b41ab022eb8"
   },
   "source": [
    "#### ***Model (2) - BERTweet-Base***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gQr3Fgv1hzyo",
   "metadata": {
    "id": "gQr3Fgv1hzyo"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model from Hugging Face\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer_bertweet_base = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the second model from HuggingFace - BERT-TWEET Transformer Encoder, fine-tuned for sentiment analysis from tweets:\n",
    "bertweet_base_2_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-base\", num_labels = 5 # 5 labels for the 5 sentiments\n",
    ").to(device)\n",
    "bertweet_base_2_model # glancing at the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08876e11-915b-4afe-9968-6602815b0d7a",
   "metadata": {
    "id": "08876e11-915b-4afe-9968-6602815b0d7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating an Optuna Study - BERTweet-Base (rec4):\n",
    "study_bertweet_base_rec4 = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function - accuracy in our case.\n",
    "study_bertweet_base_rec4.optimize(lambda trial: objective(trial, \"bertweet-base\"), n_trials=10) # Specified 10 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YCcVM8TQiwri",
   "metadata": {
    "id": "YCcVM8TQiwri"
   },
   "outputs": [],
   "source": [
    "# Documenting best hyperparameter combination - Second Model - BERTweet-Base - Rec4 code:\n",
    "\n",
    "# print(\"Best objective value (validation accuracy):\", study_bertweet_base_rec4.best_value)\n",
    "# print(\"The chosen HP combination:\", study_bertweet_base_rec4.best_params)\n",
    "# print(\"Trial number of the best objective (validation accuracy) value:\", study_bertweet_base_rec4.best_trial.number)\n",
    "\n",
    "# Define the path to save the file in Google Drive with REC4 naming\n",
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "hp_root = f\"{project_root}/Model_HPs\"\n",
    "drive_path = f\"{hp_root}/best_bertweet_base_rec4_hyperparams.json\"\n",
    "\n",
    "with open(drive_path, \"w\") as f:\n",
    "    # json.dump(study_bertweet_base_rec4.best_params, f)\n",
    "    json.dump({'learning_rate': 0.0001184412471705182, 'weight_decay': 1.2699696348040995e-05, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3}, f) # Manually typed the best_params for future use\n",
    "\n",
    "print(f\"\\nBest hyperparameters saved to {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eRjvwb0Me7Xu",
   "metadata": {
    "id": "eRjvwb0Me7Xu"
   },
   "source": [
    "## **Final Training WITHOUT using HuggingFace functions (Trainer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GY3WwP6ufV2m",
   "metadata": {
    "id": "GY3WwP6ufV2m"
   },
   "source": [
    "After finding the best trial (hyperparameter combination) using the objective function, the `FINAL_train_model_with_hyperparams` is called for final model training using the obtained hyperparameter combination. It appears similar to the way we trained each model under each trial specification in the Optuna based objective function. This additional function supports model saving too, and generalized for each model architecture. It's worth noting that in practice, the validation dataset in this function would be the actual test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nL8J3iRArTi1",
   "metadata": {
    "id": "nL8J3iRArTi1"
   },
   "outputs": [],
   "source": [
    "# basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!! (under # but the hashtag sign # can be removed if needed)\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "\n",
    "# Define model_root inside the project, for all trained weights\n",
    "model_root = f\"{project_root}/Model_Weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "esqD8VamlW_B",
   "metadata": {
    "id": "esqD8VamlW_B"
   },
   "outputs": [],
   "source": [
    "def FINAL_train_model_with_hyperparams(architecture, best_params, save_path):\n",
    "\n",
    "    # Initializing the model & tokenizer from HF, depending on the specified architecture:\n",
    "    if architecture == \"twitter-roberta-base\":\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = model.roberta\n",
    "        pretokenized_dir = (\"data/tokenized_twitter_roberta_base\")  # the folder for saving the model\n",
    "    else:\n",
    "        model_name = \"vinai/bertweet-base\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 5).to(device) # initialize RoBerta for twitter from HF, num_labels=5 -> 5 sentiments.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        base_model = model.roberta\n",
    "        pretokenized_dir = (\"data/tokenized_bertweet_base\")  # the folder for saving the model\n",
    "\n",
    "    # safety: correct dtypes + torch output\n",
    "    ds = load_from_disk(pretokenized_dir) #Loads the Arrow-backed HF DatasetDict that are defines later on in the Pre-tokenization part\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split].cast_column(\"input_ids\", Sequence(Value(\"int64\")))\n",
    "        ds[split] = ds[split].cast_column(\"attention_mask\", Sequence(Value(\"int64\")))  # or \"bool\"\n",
    "        ds[split] = ds[split].cast_column(\"labels\", Value(\"int64\"))\n",
    "        ds[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # keep dynamic padding (no tokenization here—collator only pads per batch)\n",
    "    collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "    # Merge train + validation for final training\n",
    "    full_train_dataset = concatenate_datasets([ds[\"train_reduced\"], ds[\"validation\"]])\n",
    "    full_train_dataset = full_train_dataset.shuffle(seed=42) # Shuffle the model's training data to add randomness\n",
    "\n",
    "    # build loaders from the pretokenized HF dataset\n",
    "    train_loader = DataLoader(\n",
    "        full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ds[\"test\"], batch_size=min(2*best_params[\"batch_size\"], 128), shuffle=False,\n",
    "        collate_fn=collator, num_workers=4, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    #Freezing and Unfreezing layers\n",
    "    for p in base_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if best_params[\"num_layers_finetune\"] > 0:  # safety guard: avoid the \"-0\" edge case\n",
    "        for p in base_model.encoder.layer[-best_params[\"num_layers_finetune\"]:].parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"], weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "    if wandb.run is not None:\n",
    "      wandb.finish() # Check if W&B doesn't run anything in parallel. If so, stop the pre-existing run.\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(project=f\"{architecture}_CORONA_NLP_Twitter_Sentiment_Analysis_19.8.2025_FULL_TRAINING\",\n",
    "               entity = \"idoshahar96-tel-aviv-university\",\n",
    "               config={\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"weight_decay\": best_params[\"weight_decay\"],\n",
    "        \"patience\": best_params[\"patience\"],\n",
    "        \"batch_size\": best_params[\"batch_size\"],\n",
    "        \"num_layers_finetune\": best_params[\"num_layers_finetune\"],\n",
    "        \"architecture\": architecture,\n",
    "        \"dataset\": \"CORONA-NLP-Train_Twitter-Sentiment-Analysis\"},\n",
    "        name=\"FINAL_TRAINING\", # The name that will be saved in the W&B platform\n",
    "        reinit=True)\n",
    "\n",
    "    # Train the model and get the best validation accuracy\n",
    "    best_val_accuracy = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=25, patience=best_params[\"patience\"], trial=None)\n",
    "\n",
    "    wandb.finish() # Finish the Weights & Biases run\n",
    "\n",
    "   # Save model\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31g2wIDZkkWR",
   "metadata": {
    "id": "31g2wIDZkkWR"
   },
   "outputs": [],
   "source": [
    "# best_params = study_roberta_base_tweet_rec4.best_params  # get best HPs from the model's Optuna study (are under # but the hashtag sign # can be removed if needed, for the sake of manual inscription of best param, check the row below)\n",
    "best_params = {'learning_rate': 0.0003834791389042033, 'weight_decay': 2.88286253103848e-06, 'patience': 7, 'batch_size': 128, 'num_layers_finetune': 3} # Manually typed the best_params for future use\n",
    "name_path = \"/best_model_roberta_base_tweet_rec4\"\n",
    "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
    "\n",
    "# Training the Model (1), using Optuna-study's best trial HPs - RoBERTa-Base-Tweet:\n",
    "FINAL_train_model_with_hyperparams(architecture=\"twitter-roberta-base\", best_params=best_params,save_path=save_path)\n",
    "\n",
    "# Zip the whole model folder\n",
    "shutil.make_archive(save_path, \"zip\", save_path)\n",
    "\n",
    "# Download the zip to your computer\n",
    "files.download(f\"{save_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7xoMXkViY",
   "metadata": {
    "id": "aac7xoMXkViY"
   },
   "outputs": [],
   "source": [
    "# best_params = study_bertweet_base_rec4.best_params  # get best HPs from the model's Optuna study (are under # but the hashtag sign # can be removed if needed, for the sake of manual inscription of best param, check the row below)\n",
    "best_params = {'learning_rate': 0.0001184412471705182, 'weight_decay': 1.2699696348040995e-05, 'patience': 10, 'batch_size': 128, 'num_layers_finetune': 3} # Manually typed the best_params for future use\n",
    "name_path = \"/best_model_bertweet_base_rec4\"\n",
    "save_path = model_root + name_path # initialize & define save path for the model's weights\n",
    "\n",
    "# Training the Model (2), using Optuna-study's best trial HPs - BERTweet-Base:\n",
    "FINAL_train_model_with_hyperparams(architecture=\"bertweet-base\", best_params=best_params,save_path=save_path)\n",
    "\n",
    "# Zip the whole model folder\n",
    "shutil.make_archive(save_path, \"zip\", save_path)\n",
    "\n",
    "# Download the zip to your computer\n",
    "files.download(f\"{save_path}.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
