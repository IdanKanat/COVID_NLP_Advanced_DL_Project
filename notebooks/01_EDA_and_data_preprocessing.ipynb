{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713c48a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IdanKanat/COVID_NLP_Advanced_DL_Project/blob/main/AdvancedTopicsDL_Project_IdanKanat%26IdoShahar_COVID_NLP_21.8.2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8745014",
   "metadata": {
    "id": "d8745014"
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1uGV8C_qCHmL",
   "metadata": {
    "id": "1uGV8C_qCHmL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: optuna in c:\\users\\idosh\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (1.16.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: wandb in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (0.34.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.5\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\idosh\\anaconda3\\lib\\site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idosh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install optuna\n",
    "!pip install wandb\n",
    "!pip install evaluate\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b039d84",
   "metadata": {
    "id": "9b039d84"
   },
   "outputs": [],
   "source": [
    "# Relevant imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch import nn, optim\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback, get_scheduler\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, Value, Sequence, concatenate_datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca02ad3",
   "metadata": {
    "id": "fca02ad3"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c54ac",
   "metadata": {
    "id": "636c54ac"
   },
   "source": [
    "# **Part A - Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7yOqlHzaTgsb",
   "metadata": {
    "id": "7yOqlHzaTgsb"
   },
   "source": [
    "#### Data Path (Relevant for running the files not from Drive) - **PLEASE FIRST DOWNLOAD THE [Project_COVID_NLP folder](https://drive.google.com/drive/folders/1egGGJ6F878xIk_bKUfjhyZStESiliwRC?usp=sharing) accessible from idankanat@gmail.com's Google drive!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BDFHYsAkTgGo",
   "metadata": {
    "id": "BDFHYsAkTgGo"
   },
   "outputs": [],
   "source": [
    "# Basic Drive path we used for this project. Assuming Google Colab exists as well as mounting files to drive, user can change it accordingly as he downloads the Project_COVID_NLP folder as specified in the project_root below and documented above.\n",
    "basic_drive_path = \"/content/drive/MyDrive\" # USER CAN CHANGE IT IF HE DOESN'T WORK IN DRIVE AND DOWNLOADS FROM DRIVE THE Project_COVID_NLP folder!!\n",
    "project_root = f\"{basic_drive_path}/Project_COVID_NLP\" # Root project folder\n",
    "data_path = f\"{project_root}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771e368",
   "metadata": {
    "id": "5771e368"
   },
   "outputs": [],
   "source": [
    "# Loading the Corona_NLP_train dataset:\n",
    "df = pd.read_csv(f\"{data_path}/Corona_NLP_train.csv\", encoding='latin1')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vkf5U_9pN-Jn",
   "metadata": {
    "id": "vkf5U_9pN-Jn"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587fb095",
   "metadata": {
    "id": "587fb095"
   },
   "source": [
    "### **Sentiments Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72afabf",
   "metadata": {
    "id": "d72afabf"
   },
   "outputs": [],
   "source": [
    "# Count sentiment frequencies\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Define the custom order\n",
    "custom_order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
    "\n",
    "# Reindex according to desired order\n",
    "sentiment_counts = sentiment_counts.reindex(custom_order)\n",
    "\n",
    "# Format long labels to be multi-line\n",
    "sentiment_counts.index = sentiment_counts.index.str.replace(\"Extremely Positive\", \"Extremely\\nPositive\")\n",
    "sentiment_counts.index = sentiment_counts.index.str.replace(\"Extremely Negative\", \"Extremely\\nNegative\")\n",
    "\n",
    "# Plotting the general sentiment distribution\n",
    "ax = sentiment_counts.plot(kind='bar', color='blue', edgecolor='black')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Sentiment Distribution\", fontweight='bold')\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Add top margin so numbers don't touch the edge\n",
    "plt.ylim(0, sentiment_counts.max() + 1500)\n",
    "\n",
    "# Add bold value labels for each sentiment category, with comma formatting\n",
    "for i, count in enumerate(sentiment_counts):\n",
    "    if pd.notna(count):\n",
    "        plt.text(i, count + 200, f\"{int(count):,}\", ha='center', va='bottom', fontsize=10, fontweight = \"bold\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70dce6",
   "metadata": {
    "id": "2a70dce6"
   },
   "source": [
    "From the sentiment distribution shown above, we can draw a few conclusions:\n",
    "1. **There are more positive tweets than negative tweets.**\n",
    "2. **There are more extremely positive tweets than extremely negative tweets.** This ensures that even when combining the extremes of each sentiment, positive tweets outnumber negatives. The gap between positive and negative tweets enlarges as we add the extremes of each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959189a",
   "metadata": {
    "id": "5959189a"
   },
   "source": [
    "### **Daily Tweet Counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b0ec6",
   "metadata": {
    "id": "632b0ec6"
   },
   "outputs": [],
   "source": [
    "# Standardize the 'TweetAt' date column:\n",
    "df['TweetAt'] = pd.to_datetime(df['TweetAt'], dayfirst=False, errors='coerce')\n",
    "df = df.dropna(subset=['TweetAt'])\n",
    "\n",
    "# Create a new column 'YearMonth' for grouping by month\n",
    "df['YearDay'] = df['TweetAt'].dt.date\n",
    "\n",
    "# Add a column for tweet length:\n",
    "df['TweetLength'] = df['OriginalTweet'].astype(str).apply(len)\n",
    "\n",
    "# Define sentiment colors:\n",
    "sentiment_order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
    "colors = {\n",
    "    'Extremely Negative': '#e74c3c',\n",
    "    'Negative': '#e67e22',\n",
    "    'Neutral': '#f1c40f',\n",
    "    'Positive': '#2ecc71',\n",
    "    'Extremely Positive': '#3498db',\n",
    "    'All Tweets': 'gray'\n",
    "}\n",
    "\n",
    "# First, plotting all tweets:\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "day_counts_all = df.groupby('YearDay').size()\n",
    "x_all = day_counts_all.index.astype(str)\n",
    "y_all = day_counts_all.values\n",
    "ax.plot(x_all, y_all, color='gray', marker='o', linewidth=2)\n",
    "ax.set_title(\"Daily Tweet Counts\", fontweight='bold', fontsize=14, pad=20)\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Tweet Count\")\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "# for spine in ax.spines.values():\n",
    "    # spine.set_edgecolor('red')\n",
    "    # spine.set_linewidth(3)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Second, plotting stratified sentiment trends in one plot:\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for sentiment in sentiment_order:\n",
    "    data = df[df['Sentiment'] == sentiment]\n",
    "    day_counts = data.groupby('YearDay').size()\n",
    "    x = day_counts.index.astype(str)\n",
    "    y = day_counts.values\n",
    "    ax.plot(x, y, label=sentiment, color=colors[sentiment], marker='o', linewidth=2)\n",
    "\n",
    "ax.set_title(\"Daily Tweet Counts by Sentiment\", fontweight='bold', fontsize=14, pad=20)\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Tweet Count\")\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "ax.legend(title=\"Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c4d84",
   "metadata": {
    "id": "d68c4d84"
   },
   "source": [
    "From the two plots above, we can conclude:\n",
    "- As we could intuitively predict, there was a surge of tweets in March 2020 following the COVID-19 outburst.\n",
    "- This massive surge in tweets wasn't attributed to any specific sentiment but rather all different sentiments indicated much more frequent tweets in March."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db8fe7",
   "metadata": {
    "id": "f2db8fe7"
   },
   "source": [
    "### **Tweet Length Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb461b0",
   "metadata": {
    "id": "1fb461b0"
   },
   "outputs": [],
   "source": [
    "# Compute the number of characters in each tweet\n",
    "df['TweetLength'] = df['OriginalTweet'].astype(str).str.len()\n",
    "\n",
    "# Plotting a histogram of tweet lengths:\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df['TweetLength'], bins=50, color='purple', edgecolor='black')\n",
    "\n",
    "plt.title(\"Distribution of Tweet Lengths (in characters)\", fontweight='bold')\n",
    "plt.xlabel(\"Number of Characters\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074e144",
   "metadata": {
    "id": "4074e144"
   },
   "outputs": [],
   "source": [
    "# Displaying summary statistics using the .describe() command:\n",
    "length_stats = df['TweetLength'].describe().astype(int)\n",
    "length_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76c0bd",
   "metadata": {
    "id": "7c76c0bd"
   },
   "source": [
    "From the tweet length distributions, several conclusions can be drawn:\n",
    "- **Strong right skew up to the character limit -** There’s a visible increase in tweet counts as length increases, peaking around 240–280 characters.\n",
    "\n",
    "- **A sharp drop after ~280 characters -** Reflects the Twitter character limit (likely 280) — tweets can't go longer, so the distribution is naturally cut off there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718f9dc",
   "metadata": {
    "id": "2718f9dc"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Sentiment labels and colors\n",
    "sentiment_order = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
    "colors = {\n",
    "    'Extremely Negative': '#e74c3c',\n",
    "    'Negative': '#e67e22',\n",
    "    'Neutral': '#f1c40f',\n",
    "    'Positive': '#2ecc71',\n",
    "    'Extremely Positive': '#3498db',\n",
    "    'All Tweets': 'gray'\n",
    "}\n",
    "\n",
    "# Manual plot order with 'All Tweets' in the center\n",
    "plot_order = [\n",
    "    'Extremely Negative', 'All Tweets', 'Negative',\n",
    "    'Extremely Positive',            'Neutral',   'Positive'\n",
    "]\n",
    "\n",
    "# Create 2x3 subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each distribution\n",
    "for i, label in enumerate(plot_order):\n",
    "    if label == 'All Tweets':\n",
    "        data = df['TweetLength']\n",
    "    else:\n",
    "        data = df[df['Sentiment'] == label]['TweetLength']\n",
    "\n",
    "    axes[i].hist(data, bins=40, color=colors[label], edgecolor='black', alpha=0.9)\n",
    "    axes[i].set_title(label, fontweight='bold')\n",
    "    axes[i].set_xlabel(\"Tweet Length (characters)\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add bold border to the axes itself (cleaner than external patch)\n",
    "    if label == 'All Tweets':\n",
    "        for spine in axes[i].spines.values():\n",
    "            spine.set_edgecolor('red')\n",
    "            spine.set_linewidth(3)\n",
    "\n",
    "\n",
    "# Title and layout\n",
    "plt.suptitle(\"Tweet Length Distributions by Sentiment\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5803d",
   "metadata": {
    "id": "44f5803d"
   },
   "source": [
    "Comapring the stratified distribution charts above to the general tweet-length distribution, a few insights emerge:\n",
    "1. The distributions of EACH of the non-neutral sentiments (i.e. both positive, negative, and extreme sentiments) seems to ***largely*** align with the general tweet length distribution - right skewed - i.e. a tail to the left. Long tweets are frequent.\n",
    "2. The only distinctfully different stratified histogram is w.r.t to the ***neutral*** sentiment, where shorter tweet lengths are also common, as well as the longer tweets (which are frequent in the other histograms too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492a66a",
   "metadata": {
    "id": "5492a66a"
   },
   "outputs": [],
   "source": [
    "# Ensure tweet lengths are computed\n",
    "df['TweetLength'] = df['OriginalTweet'].astype(str).str.len()\n",
    "\n",
    "# Define sentiment order + 'All'\n",
    "all_labels = sentiment_order + ['All Tweets']\n",
    "\n",
    "# Initialize dictionary to collect describe stats\n",
    "summary_dict = {}\n",
    "\n",
    "# Add describe() for each sentiment\n",
    "for sentiment in sentiment_order:\n",
    "    stats = df[df['Sentiment'] == sentiment]['TweetLength'].describe().astype(int)\n",
    "    summary_dict[sentiment] = stats\n",
    "\n",
    "# Add general (all tweets) stats\n",
    "summary_dict['All Tweets'] = df['TweetLength'].describe().astype(int)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "summary_df = pd.DataFrame(summary_dict)\n",
    "\n",
    "# Optional: Reorder rows (metrics)\n",
    "summary_df = summary_df.reindex(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "\n",
    "# Highlight function\n",
    "def highlight_extremes(row):\n",
    "    is_max = row == row.max()\n",
    "    is_min = row == row.min()\n",
    "    return ['background-color: lightgreen' if v else\n",
    "            'background-color: salmon' if m else '' for v, m in zip(is_max, is_min)]\n",
    "\n",
    "# Styling the dataframe w.r.t to row's maximum (green) & minimum (red)\n",
    "styled_df = summary_df.style.apply(highlight_extremes, axis=1)\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8608a",
   "metadata": {
    "id": "73c8608a"
   },
   "source": [
    "This table displays the key statistics of each of the stratified distributions (w.r.t to sentiment), as well as the general tweet length distribution. We can observe:\n",
    "\n",
    "- The longest tweet belongs to the extremely negative sentiment group (355 tokens!), the longest extremely positive tweet consisted of 338 tokens, indicating that tweets invoking extreme emotions appear to be longer and they're potentially POSITIVELY associated with length.\n",
    "- The neutral sentiment distribution has the largest S.D, aligning with bigger spread than the other distributions, as described above in the graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a7015",
   "metadata": {
    "id": "be6a7015"
   },
   "source": [
    "### **Tweets by Region**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c86c87",
   "metadata": {
    "id": "79c86c87"
   },
   "outputs": [],
   "source": [
    "# Load the location data\n",
    "df['Location'] = df['Location'].fillna(\"\").str.lower()  # Standardize by lowercasing all location values\n",
    "\n",
    "# Group key-words by region:\n",
    "region_keywords = {\n",
    "    \"US\": [\n",
    "        \"usa\", \"u.s.a\", \"u.s\", \"america\", \"united states of america\", \"united states\", \"texas\", \"tx\", \"austin\",\n",
    "        \"houston\", \"abilene\", \"new york\", \"new york city\", \"nyc\", \"ny\", \"california\", \"ca\", \"florida\", \"fl\",\n",
    "        \"washington\", \"dc\", \"washington dc\", \"washington d.c.\", \"alaska\", \"chicago\", \"illinois\", \"arizona\", \"az\",\n",
    "        \"atlanta\", \"ga\", \"baltimore\", \"boston\", \"brooklyn\", \"manhattan\", \"queens\", \"bronx\", \"staten island\",\n",
    "        \"il\", \"nc\", \"nj\", \"va\", \"tn\", \"oh\", \"ohio\", \"sc\", \"co\", \"colorado\", \"detroit\", \"mi\", \"hollywood\",\n",
    "        \"los angeles\", \"san fransisco\", \"honolulu\", \"hi\", \"indiana\", \"in\", \"kansas\", \"philadelphia\", \"pa\",\n",
    "        \"phoenix\", \"me\", \"or\", \"portland\", \"oregon\", \"las vegas\", \"nv\", \"maryland\", \"nevada\", \"massachusetts\",\n",
    "        \"miami\", \"michigan\", \"minneapolis\", \"nashville\", \"new orleans\", \"new jersey\", \"salt lake city\", \"ut\",\n",
    "        \"utah\", \"slc\", \"san diego\", \"seattle\", \"silicon valley\"\n",
    "    ],\n",
    "    \"UK & Commonwealth\": [\n",
    "        \"england\", \"uk\", \"u.k\", \"united kingdom\", \"london\", \"essex\", \"leeds\", \"liverpool\", \"manchester\",\n",
    "        \"canada\", \"toronto\", \"ontario\", \"alberta\", \"british columbia\", \"montreal\", \"quebec\", \"ottawa\", \"vancouver\",\n",
    "        \"australia\", \"south australia\", \"canberra\", \"melbourne\", \"sydney\", \"adelaide\", \"victoria\",\n",
    "        \"new zealand\", \"auckland\", \"scotland\", \"aberdeen\", \"edinburgh\", \"glasgow\", \"ireland\", \"dublin\"\n",
    "    ],\n",
    "    \"Europe\": [\n",
    "        \"netherlands\", \"amsterdam\", \"nederland\", \"holland\", \"the netherlands\",\n",
    "        \"germany\", \"berlin\", \"frankfurt\", \"munich\", \"hamburg\", \"dusseldorf\", \"deutschland\",\n",
    "        \"france\", \"paris\", \"belgium\", \"brussels\", \"switzerland\", \"geneva\", \"zurich\",\n",
    "        \"spain\", \"barcelona\", \"madrid\", \"italy\", \"milan\", \"milano\", \"rome\", \"roma\",\n",
    "        \"portugal\", \"lisbon\", \"austria\", \"vienna\", \"russia\", \"moscow\", \"st. petersburg\"\n",
    "    ],\n",
    "    \"Africa\": [\n",
    "        \"south africa\", \"cape town\", \"johannesburg\", \"ghana\", \"accra\", \"nigeria\", \"lagos\",\n",
    "        \"kenya\", \"uganda\", \"kampala\"\n",
    "    ],\n",
    "    \"Asia\": [\n",
    "        \"india\", \"mumbai\", \"new delhi\", \"delhi\", \"bangalore\", \"hong kong\", \"singapore\",\n",
    "        \"japan\", \"tokyo\", \"pakistan\", \"malaysia\", \"china\", \"shanghai\",\n",
    "        \"united arab emirates\", \"united arab emirate\", \"abu dhabi\", \"uae\", \"dubai\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Reverse mapping: from keyword to region\n",
    "keyword_to_region = {\n",
    "    keyword: region for region, keywords in region_keywords.items() for keyword in keywords\n",
    "}\n",
    "\n",
    "# Assigning region per location using this function:\n",
    "def assign_region(location):\n",
    "    for keyword, region in keyword_to_region.items():\n",
    "        if keyword in location:\n",
    "            return region\n",
    "    return None\n",
    "\n",
    "# Map the locations to their corresponding defined regions:\n",
    "df['Region'] = df['Location'].apply(assign_region)\n",
    "\n",
    "# Count tweets per region:\n",
    "region_counts = df['Region'].value_counts().reset_index()\n",
    "region_counts.columns = ['Region', 'TweetCount']\n",
    "\n",
    "# Plotting the pie chart:\n",
    "plt.figure(figsize=(9, 9))\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    region_counts['TweetCount'],\n",
    "    labels=[f\"{region} ({count})\" for region, count in zip(region_counts['Region'], region_counts['TweetCount'])],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140,\n",
    "    pctdistance=0.65,       # Move percentage labels further inward\n",
    "    labeldistance=1.15,     # Move labels further out\n",
    "    textprops={'fontsize': 12}\n",
    ")\n",
    "\n",
    "plt.title(\"Tweet Distribution by Region\", fontsize=14, pad=35)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef0248",
   "metadata": {
    "id": "70ef0248"
   },
   "source": [
    "The dataset contains 41,158 tweets, but there are 8,594 tweets without location values (~20%).\n",
    "\n",
    "\n",
    "So, out of the remaining 32,564 tweets with location values, we analyzed the location distribution of **86% of them (28,095 tweets)** as shown in the above pie chart.\n",
    "\n",
    "The other 14% were non-indicative locations (gibberish, small & irrelevant cities without countries mentioned, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9bd26",
   "metadata": {
    "id": "14c9bd26"
   },
   "source": [
    "### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y0urSsS6eYq7",
   "metadata": {
    "id": "Y0urSsS6eYq7"
   },
   "source": [
    "To reduce noise in the tweet content, we prepared the Corona_NLP dataset (train & test) for sentiment analysis by standardizing the tweet text. This included:\n",
    "\n",
    "- Expanded English contractions (e.g., don’t → do not) to standardize wording.\n",
    "\n",
    "- Replaced URLs and user mentions with placeholders, while simplifying hashtags.\n",
    "\n",
    "- Removed unnecessary punctuation and normalized whitespace.\n",
    "\n",
    "- Lowercased text to ensure consistency across tokens.\n",
    "\n",
    "The clean versions of the train & test datasets with an added CleanTweet column, were saved as new CSV files for further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f761e2-819c-42c0-9606-d4d8aab827e6",
   "metadata": {
    "id": "e2f761e2-819c-42c0-9606-d4d8aab827e6"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{data_path}/Corona_NLP_train.csv\", encoding='latin1')\n",
    "test_df = pd.read_csv(f\"{data_path}/Corona_NLP_test.csv\", encoding='latin1')\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Minimal set to avoid external libs. Non-destructive if not present.\n",
    "# We will apply this function inside the next function (clean_tweet_sentiment_friendly)\n",
    "def basic_contractions_expand(text: str) -> str:\n",
    "    mapping = {\n",
    "        \"can't\": \"can not\", \"won't\": \"will not\", \"don't\": \"do not\", \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\", \"i'm\": \"i am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\", \"they're\": \"they are\", \"we're\": \"we are\", \"you're\": \"you are\",\n",
    "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"shouldn't\": \"should not\", \"couldn't\": \"could not\", \"wouldn't\": \"would not\",\n",
    "        \"i've\": \"i have\", \"we've\": \"we have\", \"they've\": \"they have\", \"who's\": \"who is\",\n",
    "        \"what's\": \"what is\", \"let's\": \"let us\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "        \"he's\": \"he is\", \"she's\": \"she is\"\n",
    "    }\n",
    "    # Replace using regex with word boundaries, case-insensitive\n",
    "    def repl(m):\n",
    "        s = m.group(0)\n",
    "        return mapping.get(s.lower(), s)\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, mapping.keys())) + r\")\\b\", flags=re.IGNORECASE)\n",
    "    return pattern.sub(repl, text)\n",
    "\n",
    "#the main cleaning function of the dataset\n",
    "def clean_tweet_sentiment_friendly(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    t = text     # Preserve original sentiment cues as much as possible\n",
    "    t = basic_contractions_expand(t)\n",
    "    t = re.sub(r'http\\S+|www\\.\\S+', ' URL ', t)             # URLs -> token\n",
    "    t = re.sub(r'(?<=\\s)RT\\s+', ' ', t)                     # RT markers (Retweet sign) at word boundary\n",
    "    t = re.sub(r'@\\w+', ' @user ', t)                       # Mentions -> @user\n",
    "    t = re.sub(r'#(\\w+)', r'\\1', t)                         # Remove hashtags but keep hashtag word\n",
    "    t = re.sub(r\"[\\\"$%^&*()\\-_=+\\[\\]{};:|/\\\\<>]\", \" \", t)   # strip most punctuation and special characters, BESIDE ! and ?\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()                      # Normalize whitespaces to only one whitespace\n",
    "    t = t.lower()                                           # Lowercase\n",
    "    return t\n",
    "\n",
    "\n",
    "clean_train = train_df.copy()\n",
    "clean_train[\"CleanTweet\"] = clean_train[\"OriginalTweet\"].apply(clean_tweet_sentiment_friendly)\n",
    "clean_test = test_df.copy()\n",
    "clean_test[\"CleanTweet\"] = clean_test[\"OriginalTweet\"].apply(clean_tweet_sentiment_friendly)\n",
    "clean_train.to_csv(f\"{data_path}/CLEAN_Corona_NLP_train.csv\", index=False, encoding=\"utf-8\")\n",
    "clean_test.to_csv(f\"{data_path}/CLEAN_Corona_NLP_test.csv\",  index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Basic inspections of the cleaned train & test datasets:\n",
    "clean_train.head(200)\n",
    "clean_test.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da17ba6",
   "metadata": {
    "id": "7da17ba6"
   },
   "source": [
    "# **Part B - Training Pre-Trained HuggingFace models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a74f4-3bbb-4330-b317-d5a941205a34",
   "metadata": {
    "id": "eb0a74f4-3bbb-4330-b317-d5a941205a34"
   },
   "source": [
    "### **Data Splitting - Train and Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PInTPvp7f-4t",
   "metadata": {
    "id": "PInTPvp7f-4t"
   },
   "source": [
    "We split the original training dataset into training and validation subsets, ensuring stratification which respects the original label / sentiment distributions, now in the new subsets. Besides, the new subsets contained only relevant info for classification, i.e. the cleaned tweet content and the labels / sentiments themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4dec4-4362-44b6-9bdb-6cbf1f96bd85",
   "metadata": {
    "id": "1ac4dec4-4362-44b6-9bdb-6cbf1f96bd85"
   },
   "outputs": [],
   "source": [
    "# load the CLEAN datasets using ISO-8859-1 encoding due to UTF-8 decoding error\n",
    "# We are loading the only two relevant columns for the classification (Sentiment label & Cleaned Tweet content)\n",
    "train_df = pd.read_csv(f\"{data_path}/CLEAN_Corona_NLP_train.csv\", encoding='latin1',usecols=[\"Sentiment\", \"CleanTweet\"] )\n",
    "test_df = pd.read_csv(f\"{data_path}/CLEAN_Corona_NLP_test.csv\", encoding='latin1', usecols=[\"Sentiment\", \"CleanTweet\"])\n",
    "\n",
    "# Fixed label mapping for all of the data before splitting\n",
    "label_order = ['Extremely Negative','Negative','Neutral','Positive','Extremely Positive']\n",
    "label2id = {l:i for i,l in enumerate(label_order)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "test_size = len(test_df)\n",
    "\n",
    "# Split the training data to create a validation set of the same size as the test set, stratification was included to keep the same label distribution across the training & validation subsets\n",
    "train_df_reduced, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=test_size, # Validation set's size equals the test set's size\n",
    "    random_state=42,\n",
    "    stratify=train_df['Sentiment'] # Stratified the subsets w.r.t labels - sentiments\n",
    ")\n",
    "\n",
    "train_df_reduced_size = len(train_df_reduced)\n",
    "val_size = len(val_df)\n",
    "total = train_df_reduced_size + val_size + test_size\n",
    "\n",
    "# Create a summary table\n",
    "summary = {\n",
    "    \"Dataset\": [\"Training after splitting\", \"Validation\", \"Test\"],\n",
    "    \"Records\": [train_df_reduced_size, val_size, test_size],\n",
    "    \"Percentage\": [round(100 * train_df_reduced_size / total, 2),\n",
    "                   round(100 * val_size / total, 2),\n",
    "                   round(100 * test_size / total, 2)]\n",
    "}\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
